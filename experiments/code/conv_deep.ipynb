{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Total parameters: 77,836,105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 313/313 [00:49<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.00285, Color Loss: 0.02435, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1: 100%|██████████| 40/40 [00:03<00:00, 12.48it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00303, Color Loss: 0.02621\n",
      "New best model saved with val loss 0.00303 and color loss 0.02621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 313/313 [00:48<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.00250, Color Loss: 0.02264, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2: 100%|██████████| 40/40 [00:03<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00285, Color Loss: 0.02522\n",
      "New best model saved with val loss 0.00285 and color loss 0.02522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 313/313 [00:48<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.00245, Color Loss: 0.02231, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 3: 100%|██████████| 40/40 [00:03<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00292, Color Loss: 0.02560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.00260, Color Loss: 0.02275, LR: 9.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 4: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00288, Color Loss: 0.02534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.00218, Color Loss: 0.02079, LR: 9.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 5: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00280, Color Loss: 0.02494\n",
      "New best model saved with val loss 0.00280 and color loss 0.02494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.00256, Color Loss: 0.02285, LR: 9.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 6: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00276, Color Loss: 0.02475\n",
      "New best model saved with val loss 0.00276 and color loss 0.02475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.00224, Color Loss: 0.02101, LR: 9.91e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 7: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00277, Color Loss: 0.02490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.00210, Color Loss: 0.02046, LR: 9.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 8: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00271, Color Loss: 0.02445\n",
      "New best model saved with val loss 0.00271 and color loss 0.02445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.00231, Color Loss: 0.02155, LR: 9.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 9: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00270, Color Loss: 0.02441\n",
      "New best model saved with val loss 0.00270 and color loss 0.02441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.00218, Color Loss: 0.02085, LR: 9.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 10: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00270, Color Loss: 0.02438\n",
      "New best model saved with val loss 0.00270 and color loss 0.02438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Avg Loss: 0.00223, Color Loss: 0.02077, LR: 9.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 11: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00265, Color Loss: 0.02409\n",
      "New best model saved with val loss 0.00265 and color loss 0.02409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Avg Loss: 0.00216, Color Loss: 0.02058, LR: 9.70e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 12: 100%|██████████| 40/40 [00:03<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00257, Color Loss: 0.02386\n",
      "New best model saved with val loss 0.00257 and color loss 0.02386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Avg Loss: 0.00198, Color Loss: 0.01961, LR: 9.65e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 13: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00266, Color Loss: 0.02416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Avg Loss: 0.00209, Color Loss: 0.02012, LR: 9.59e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 14: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00264, Color Loss: 0.02411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Avg Loss: 0.00191, Color Loss: 0.01923, LR: 9.52e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 15: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00263, Color Loss: 0.02395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 313/313 [00:49<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Avg Loss: 0.00192, Color Loss: 0.01920, LR: 9.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 16: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00269, Color Loss: 0.02421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Avg Loss: 0.00183, Color Loss: 0.01877, LR: 9.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 17: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00258, Color Loss: 0.02379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Avg Loss: 0.00162, Color Loss: 0.01765, LR: 9.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 18: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00261, Color Loss: 0.02376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Avg Loss: 0.00193, Color Loss: 0.01925, LR: 9.22e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 19: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00262, Color Loss: 0.02383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Avg Loss: 0.00159, Color Loss: 0.01753, LR: 9.14e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 20: 100%|██████████| 40/40 [00:03<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00258, Color Loss: 0.02367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Avg Loss: 0.00173, Color Loss: 0.01818, LR: 9.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 21: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00260, Color Loss: 0.02366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Avg Loss: 0.00146, Color Loss: 0.01673, LR: 8.95e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 22: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00256, Color Loss: 0.02342\n",
      "New best model saved with val loss 0.00256 and color loss 0.02342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Avg Loss: 0.00154, Color Loss: 0.01722, LR: 8.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 23: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00265, Color Loss: 0.02394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Avg Loss: 0.00152, Color Loss: 0.01695, LR: 8.75e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 24: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00251, Color Loss: 0.02328\n",
      "New best model saved with val loss 0.00251 and color loss 0.02328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Avg Loss: 0.00163, Color Loss: 0.01728, LR: 8.64e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 25: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00259, Color Loss: 0.02388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Avg Loss: 0.00143, Color Loss: 0.01635, LR: 8.54e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 26: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00261, Color Loss: 0.02383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Avg Loss: 0.00153, Color Loss: 0.01687, LR: 8.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 27: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00256, Color Loss: 0.02357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Avg Loss: 0.00142, Color Loss: 0.01640, LR: 8.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 28: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00251, Color Loss: 0.02326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Avg Loss: 0.00153, Color Loss: 0.01674, LR: 8.19e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 29: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00247, Color Loss: 0.02312\n",
      "New best model saved with val loss 0.00247 and color loss 0.02312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Avg Loss: 0.00140, Color Loss: 0.01605, LR: 8.06e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 30: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00261, Color Loss: 0.02390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Avg Loss: 0.00147, Color Loss: 0.01655, LR: 7.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 31: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00250, Color Loss: 0.02336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Avg Loss: 0.00142, Color Loss: 0.01625, LR: 7.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 32: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00243, Color Loss: 0.02287\n",
      "New best model saved with val loss 0.00243 and color loss 0.02287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Avg Loss: 0.00127, Color Loss: 0.01565, LR: 7.68e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 33: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00239, Color Loss: 0.02268\n",
      "New best model saved with val loss 0.00239 and color loss 0.02268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.00140, Color Loss: 0.01606, LR: 7.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 34: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00239, Color Loss: 0.02270\n",
      "New best model saved with val loss 0.00239 and color loss 0.02270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.00122, Color Loss: 0.01518, LR: 7.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 35: 100%|██████████| 40/40 [00:03<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00242, Color Loss: 0.02291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.00132, Color Loss: 0.01572, LR: 7.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 36: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00237, Color Loss: 0.02261\n",
      "New best model saved with val loss 0.00237 and color loss 0.02261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.00119, Color Loss: 0.01488, LR: 7.13e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 37: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00241, Color Loss: 0.02294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.00110, Color Loss: 0.01437, LR: 6.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 38: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00245, Color Loss: 0.02301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 0.00124, Color Loss: 0.01525, LR: 6.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 39: 100%|██████████| 40/40 [00:03<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00236, Color Loss: 0.02259\n",
      "New best model saved with val loss 0.00236 and color loss 0.02259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Avg Loss: 0.00109, Color Loss: 0.01433, LR: 6.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 40: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00237, Color Loss: 0.02263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Avg Loss: 0.00113, Color Loss: 0.01456, LR: 6.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 41: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00241, Color Loss: 0.02275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Avg Loss: 0.00117, Color Loss: 0.01474, LR: 6.39e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 42: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00238, Color Loss: 0.02264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Avg Loss: 0.00109, Color Loss: 0.01419, LR: 6.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 43: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00250, Color Loss: 0.02321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Avg Loss: 0.00103, Color Loss: 0.01393, LR: 6.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 44: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00242, Color Loss: 0.02288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Avg Loss: 0.00100, Color Loss: 0.01383, LR: 5.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 45: 100%|██████████| 40/40 [00:03<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00244, Color Loss: 0.02289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Avg Loss: 0.00099, Color Loss: 0.01363, LR: 5.78e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 46: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00245, Color Loss: 0.02300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Avg Loss: 0.00110, Color Loss: 0.01426, LR: 5.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 47: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00230, Color Loss: 0.02229\n",
      "New best model saved with val loss 0.00230 and color loss 0.02229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Avg Loss: 0.00105, Color Loss: 0.01407, LR: 5.47e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 48: 100%|██████████| 40/40 [00:03<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00235, Color Loss: 0.02249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Avg Loss: 0.00098, Color Loss: 0.01383, LR: 5.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 49: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00238, Color Loss: 0.02267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Avg Loss: 0.00110, Color Loss: 0.01424, LR: 5.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 50: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00239, Color Loss: 0.02264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - Avg Loss: 0.00110, Color Loss: 0.01425, LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 51: 100%|██████████| 40/40 [00:03<00:00, 12.57it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00237, Color Loss: 0.02251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 - Avg Loss: 0.00109, Color Loss: 0.01417, LR: 4.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 52: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00236, Color Loss: 0.02256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - Avg Loss: 0.00120, Color Loss: 0.01477, LR: 4.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 53: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00234, Color Loss: 0.02243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 - Avg Loss: 0.00134, Color Loss: 0.01554, LR: 4.53e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 54: 100%|██████████| 40/40 [00:03<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00235, Color Loss: 0.02253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 - Avg Loss: 0.00101, Color Loss: 0.01382, LR: 4.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 55: 100%|██████████| 40/40 [00:03<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00230, Color Loss: 0.02214\n",
      "New best model saved with val loss 0.00230 and color loss 0.02214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 - Avg Loss: 0.00091, Color Loss: 0.01317, LR: 4.22e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 56: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00237, Color Loss: 0.02266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 - Avg Loss: 0.00097, Color Loss: 0.01364, LR: 4.06e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 57: 100%|██████████| 40/40 [00:03<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00236, Color Loss: 0.02241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 - Avg Loss: 0.00104, Color Loss: 0.01402, LR: 3.91e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 58: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59: 100%|██████████| 313/313 [00:49<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 - Avg Loss: 0.00103, Color Loss: 0.01398, LR: 3.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 59: 100%|██████████| 40/40 [00:03<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00237, Color Loss: 0.02251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 - Avg Loss: 0.00095, Color Loss: 0.01353, LR: 3.61e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 60: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 - Avg Loss: 0.00097, Color Loss: 0.01353, LR: 3.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 61: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 62: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 - Avg Loss: 0.00106, Color Loss: 0.01409, LR: 3.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 62: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00230, Color Loss: 0.02216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 63: 100%|██████████| 313/313 [00:49<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 - Avg Loss: 0.00097, Color Loss: 0.01353, LR: 3.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 63: 100%|██████████| 40/40 [00:03<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00234, Color Loss: 0.02236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 64: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 - Avg Loss: 0.00103, Color Loss: 0.01398, LR: 3.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 64: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00229, Color Loss: 0.02209\n",
      "New best model saved with val loss 0.00229 and color loss 0.02209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 65: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 - Avg Loss: 0.00098, Color Loss: 0.01347, LR: 2.87e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 65: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00236, Color Loss: 0.02242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 66: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 - Avg Loss: 0.00100, Color Loss: 0.01367, LR: 2.73e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 66: 100%|██████████| 40/40 [00:03<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00233, Color Loss: 0.02236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 67: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 - Avg Loss: 0.00110, Color Loss: 0.01415, LR: 2.59e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 67: 100%|██████████| 40/40 [00:03<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00229, Color Loss: 0.02217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 68: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 - Avg Loss: 0.00104, Color Loss: 0.01394, LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 68: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00231, Color Loss: 0.02221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 69: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 - Avg Loss: 0.00089, Color Loss: 0.01315, LR: 2.32e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 69: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00240, Color Loss: 0.02260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 70: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 - Avg Loss: 0.00105, Color Loss: 0.01401, LR: 2.19e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 70: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00235, Color Loss: 0.02246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 71: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 - Avg Loss: 0.00101, Color Loss: 0.01381, LR: 2.06e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 71: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 72: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 - Avg Loss: 0.00093, Color Loss: 0.01337, LR: 1.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 72: 100%|██████████| 40/40 [00:03<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00236, Color Loss: 0.02243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 73: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 - Avg Loss: 0.00105, Color Loss: 0.01398, LR: 1.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 73: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00228, Color Loss: 0.02207\n",
      "New best model saved with val loss 0.00228 and color loss 0.02207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 74: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 - Avg Loss: 0.00100, Color Loss: 0.01365, LR: 1.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 74: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00231, Color Loss: 0.02221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 75: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 - Avg Loss: 0.00107, Color Loss: 0.01400, LR: 1.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 75: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00228, Color Loss: 0.02206\n",
      "New best model saved with val loss 0.00228 and color loss 0.02206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 76: 100%|██████████| 313/313 [00:49<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 - Avg Loss: 0.00104, Color Loss: 0.01390, LR: 1.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 76: 100%|██████████| 40/40 [00:03<00:00, 12.61it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00225, Color Loss: 0.02196\n",
      "New best model saved with val loss 0.00225 and color loss 0.02196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 77: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 - Avg Loss: 0.00102, Color Loss: 0.01389, LR: 1.36e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 77: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 78: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 - Avg Loss: 0.00108, Color Loss: 0.01411, LR: 1.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 78: 100%|██████████| 40/40 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00228, Color Loss: 0.02210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 79: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 - Avg Loss: 0.00097, Color Loss: 0.01355, LR: 1.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 79: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00231, Color Loss: 0.02228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 80: 100%|██████████| 313/313 [00:49<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 - Avg Loss: 0.00090, Color Loss: 0.01317, LR: 1.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 80: 100%|██████████| 40/40 [00:03<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00229, Color Loss: 0.02211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 81: 100%|██████████| 313/313 [00:49<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 - Avg Loss: 0.00103, Color Loss: 0.01377, LR: 9.55e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 81: 100%|██████████| 40/40 [00:03<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 82: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 - Avg Loss: 0.00103, Color Loss: 0.01383, LR: 8.65e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 82: 100%|██████████| 40/40 [00:03<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 83: 100%|██████████| 313/313 [00:49<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 - Avg Loss: 0.00097, Color Loss: 0.01353, LR: 7.78e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 83: 100%|██████████| 40/40 [00:03<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00233, Color Loss: 0.02232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 84: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 - Avg Loss: 0.00094, Color Loss: 0.01333, LR: 6.96e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 84: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00226, Color Loss: 0.02203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 85: 100%|██████████| 313/313 [00:49<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 - Avg Loss: 0.00107, Color Loss: 0.01399, LR: 6.18e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 85: 100%|██████████| 40/40 [00:03<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00232, Color Loss: 0.02228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 86: 100%|██████████| 313/313 [00:49<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 - Avg Loss: 0.00096, Color Loss: 0.01340, LR: 5.45e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 86: 100%|██████████| 40/40 [00:03<00:00, 12.65it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00238, Color Loss: 0.02242\n",
      "Early stopping after 86 epochs!\n",
      "Training completed!\n",
      "Loaded best model from epoch 76 with val loss 0.00225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 403.71it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 419.40it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 415.15it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 416.83it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 414.16it/s]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 361.82it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 418.08it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 416.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restoration Quality Metrics:\n",
      "--------------------------------------------------\n",
      "Quality   PSNR (dB)      Color Loss     \n",
      "--------------------------------------------------\n",
      "10        -26.86         0.36525        \n",
      "30        -23.68         0.22981        \n",
      "50        11.22          0.07338        \n",
      "70        17.94          0.04242        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 資料預處理\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size]\n",
    ")\n",
    "\n",
    "# 資料載入器設定\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 改進的JPEG壓縮函數\n",
    "def jpeg_compress(x, quality):\n",
    "    \"\"\"執行JPEG壓縮並提升色彩保存效果\"\"\"\n",
    "    x = (x * 127.5 + 127.5).clamp(0, 255).to(torch.uint8).cpu()\n",
    "    compressed_images = []\n",
    "    for img in x:\n",
    "        pil_img = torchvision.transforms.ToPILImage()(img)\n",
    "        buffer = io.BytesIO()\n",
    "        # 使用4:4:4色度採樣保持更好的色彩信息，除非質量非常低\n",
    "        subsampling = \"4:4:4\" if quality > 30 else \"4:2:0\"\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=quality, subsampling=subsampling)\n",
    "        buffer.seek(0)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        compressed_tensor = torchvision.transforms.ToTensor()(compressed_img)\n",
    "        compressed_images.append(compressed_tensor)\n",
    "    return torch.stack(compressed_images).to(device).sub(0.5).div(0.5)\n",
    "\n",
    "# 色彩損失函數\n",
    "def color_loss(x, y):\n",
    "    \"\"\"計算色彩損失，著重在顏色變化而非亮度\"\"\"\n",
    "    # 轉換到YCbCr空間的近似\n",
    "    x_rgb = (x * 0.5 + 0.5).clamp(0, 1)\n",
    "    y_rgb = (y * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    # 計算色度通道的差異，給予更高的權重\n",
    "    color_diff = torch.abs(x_rgb - y_rgb)\n",
    "    \n",
    "    # 不同通道的權重 - 給色彩通道更高權重\n",
    "    r_weight, g_weight, b_weight = 0.25, 0.5, 0.25\n",
    "    weighted_diff = color_diff[:, 0:1] * r_weight + color_diff[:, 1:2] * g_weight + color_diff[:, 2:3] * b_weight\n",
    "    \n",
    "    return weighted_diff.mean()\n",
    "\n",
    "# 時間嵌入模組\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.proj(emb)\n",
    "\n",
    "# 殘差注意力模塊\n",
    "class ResAttnBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 確保組數能被通道數整除\n",
    "        num_groups = min(8, in_c)\n",
    "        while in_c % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "            \n",
    "        self.norm1 = nn.GroupNorm(num_groups, in_c)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.time_proj = nn.Linear(time_dim, out_c)\n",
    "        \n",
    "        # 調整 out_c 的組數\n",
    "        num_groups_out = min(8, out_c)\n",
    "        while out_c % num_groups_out != 0 and num_groups_out > 1:\n",
    "            num_groups_out -= 1\n",
    "            \n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.attn = nn.MultiheadAttention(out_c, 4, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        # 加入時間編碼\n",
    "        t = self.time_proj(t_emb)[..., None, None]\n",
    "        h = h + t\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = self.conv2(F.silu(h))\n",
    "        \n",
    "        # 應用自注意力機制\n",
    "        b, c, hh, ww = h.shape\n",
    "        h_attn = h.view(b, c, -1).permute(0, 2, 1)\n",
    "        h_attn, _ = self.attn(h_attn, h_attn, h_attn)\n",
    "        h_attn = h_attn.permute(0, 2, 1).view(b, c, hh, ww)\n",
    "        \n",
    "        return self.shortcut(x) + self.dropout(h_attn)\n",
    "\n",
    "# 改進的UNet架構\n",
    "class JPEGDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        time_dim = 256\n",
    "        self.time_embed = TimeEmbedding(time_dim)\n",
    "        \n",
    "        # 下采样路径 - 增强通道数以提高色彩保存能力\n",
    "        self.down1 = ResAttnBlock(3, 64, time_dim)\n",
    "        self.down2 = ResAttnBlock(64, 128, time_dim)\n",
    "        self.down3 = ResAttnBlock(128, 256, time_dim)\n",
    "        self.down4 = ResAttnBlock(256, 512, time_dim)\n",
    "        self.down5 = ResAttnBlock(512, 512, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 瓶颈层\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResAttnBlock(512, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 512, time_dim)\n",
    "        )\n",
    "        \n",
    "        # 上采样路径\n",
    "        self.up1 = ResAttnBlock(1024, 512, time_dim)\n",
    "        self.up2 = ResAttnBlock(512 + 512, 256, time_dim)\n",
    "        self.up3 = ResAttnBlock(256 + 256, 128, time_dim)\n",
    "        self.up4 = ResAttnBlock(128 + 128, 64, time_dim)\n",
    "        self.up5 = ResAttnBlock(64 + 64, 64, time_dim)\n",
    "        \n",
    "        # 输出层 - 使用1x1卷积保留空间色彩相关性\n",
    "        self.out_conv = nn.Conv2d(64, 3, 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # 下采样\n",
    "        d1 = self.down1(x, t_emb)  # 32x32\n",
    "        d2 = self.down2(self.pool(d1), t_emb)  # 16x16\n",
    "        d3 = self.down3(self.pool(d2), t_emb)  # 8x8\n",
    "        d4 = self.down4(self.pool(d3), t_emb)  # 4x4\n",
    "        d5 = self.down5(self.pool(d4), t_emb)  # 2x2\n",
    "        \n",
    "        # 瓶颈层\n",
    "        b = self.bottleneck[0](self.pool(d5), t_emb)\n",
    "        b = self.bottleneck[1](b, t_emb)\n",
    "        b = self.bottleneck[2](b, t_emb)\n",
    "        \n",
    "        # 上采样 - 使用双线性上采样避免棋盘格效应\n",
    "        u1 = self.up1(torch.cat([F.interpolate(b, scale_factor=2, mode='bilinear', align_corners=False), d5], dim=1), t_emb)\n",
    "        u2 = self.up2(torch.cat([F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False), d4], dim=1), t_emb)\n",
    "        u3 = self.up3(torch.cat([F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False), d3], dim=1), t_emb)\n",
    "        u4 = self.up4(torch.cat([F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False), d2], dim=1), t_emb)\n",
    "        u5 = self.up5(torch.cat([F.interpolate(u4, scale_factor=2, mode='bilinear', align_corners=False), d1], dim=1), t_emb)\n",
    "        \n",
    "        return self.out_conv(u5)\n",
    "\n",
    "# 初始化模型\n",
    "model = JPEGDiffusionModel().to(device)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 設定優化器 - 使用AdamW配合學習率調整\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5, betas=(0.9, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)\n",
    "loss_fn = nn.HuberLoss(reduction='mean', delta=1.0)  # 使用Huber損失，對於非高斯噪聲更穩健\n",
    "\n",
    "# 設定JPEG擴散參數\n",
    "num_timesteps = 100\n",
    "# 根據論文，使用余弦調度的噪聲\n",
    "betas = torch.linspace(1e-4, 0.02, num_timesteps).to(device)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# 高斯混合模型採樣器\n",
    "class GaussianMixtureSampler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def optimize_mixture_params(self, x_t, pred_noise, t_step, t_next):\n",
    "        \"\"\"優化高斯混合模型參數\"\"\"\n",
    "        # 預測的x0\n",
    "        x0_pred = x_t + pred_noise\n",
    "        \n",
    "        # 使用預測的x0計算雙峰高斯分布的均值\n",
    "        # 第一個均值 - 更傾向於保持原始預測\n",
    "        mu1 = x0_pred * 0.9 + x_t * 0.1\n",
    "        \n",
    "        # 第二個均值 - 更傾向於向原始圖像方向移動\n",
    "        mu2 = x0_pred * 1.1 - x_t * 0.1\n",
    "        \n",
    "        # 估算標準差 - 隨時間變化\n",
    "        # 接近結束時使用較小的標準差\n",
    "        time_weight = t_step / num_timesteps\n",
    "        sigma_base = 0.15 * time_weight\n",
    "        \n",
    "        return mu1, mu2, sigma_base\n",
    "        \n",
    "    def sample(self, x_t, steps=100, guidance_scale=1.0):\n",
    "        \"\"\"使用高斯混合模型進行採樣\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 從給定的噪聲圖像開始\n",
    "            for i in tqdm(range(steps-1, -1, -1), desc=\"Sampling\"):\n",
    "                t = torch.full((x_t.size(0),), i, device=device).float() / num_timesteps\n",
    "                \n",
    "                # 獲取噪聲預測\n",
    "                pred_noise = self.model(x_t, t)\n",
    "                \n",
    "                if i > 0:\n",
    "                    # 計算高斯混合模型參數\n",
    "                    mu1, mu2, sigma = self.optimize_mixture_params(x_t, pred_noise, i, i-1)\n",
    "                    \n",
    "                    # 隨機選擇使用哪個高斯分量\n",
    "                    if random.random() < 0.33:  # 1/3概率使用第一個均值\n",
    "                        next_mean = mu1\n",
    "                    else:  # 2/3概率使用第二個均值\n",
    "                        next_mean = mu2\n",
    "                    \n",
    "                    # 隨機性逐漸減少，接近原始圖像時幾乎無隨機性\n",
    "                    noise_scale = sigma * (1.0 - (steps - i) / steps) * guidance_scale\n",
    "                    \n",
    "                    # 下一步\n",
    "                    x_t = next_mean + torch.randn_like(x_t) * noise_scale\n",
    "                else:\n",
    "                    # 最後一步直接使用預測的原始圖像\n",
    "                    x_t = x_t + pred_noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "# 改進的訓練函數\n",
    "def train_epoch(model, loader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    color_loss_total = 0\n",
    "    \n",
    "    for x0, _ in tqdm(loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        x0 = x0.to(device)\n",
    "        b = x0.size(0)\n",
    "        \n",
    "        # 使用結構化的質量分布\n",
    "        # 確保模型見到各種質量水平的樣本，尤其是高質量\n",
    "        if random.random() < 0.4 + min(0.4, epoch * 0.01):  # 隨著訓練進行增加高質量樣本比例\n",
    "            # 高質量壓縮\n",
    "            quality_range = (70, 100)\n",
    "        elif random.random() < 0.6:\n",
    "            # 中等質量壓縮\n",
    "            quality_range = (40, 70)\n",
    "        else:\n",
    "            # 低質量壓縮\n",
    "            quality_range = (5, 40)\n",
    "            \n",
    "        # 隨機選擇時間步\n",
    "        t = torch.randint(1, num_timesteps, (b,), device=device).long()\n",
    "        \n",
    "        # 將時間步映射到質量因子，加入範圍控制\n",
    "        min_q, max_q = quality_range\n",
    "        quality = torch.clamp(min_q + (max_q - min_q) * (1 - t.float() / num_timesteps), 1, 100).cpu().numpy()\n",
    "        \n",
    "        # 對每個樣本使用不同質量進行JPEG壓縮\n",
    "        xt = torch.stack([jpeg_compress(x0[i:i+1], int(q)) for i, q in enumerate(quality)]).squeeze()\n",
    "        \n",
    "        # 計算噪聲 (x0 - xt)\n",
    "        noise = x0 - xt\n",
    "        \n",
    "        # 模型預測噪聲\n",
    "        pred_noise = model(xt, t.float()/num_timesteps)\n",
    "        \n",
    "        # 計算損失 - 組合標準損失和色彩損失\n",
    "        main_loss = loss_fn(pred_noise, noise)\n",
    "        col_loss = color_loss(x0, xt + pred_noise)\n",
    "        \n",
    "        # 隨著訓練進行，逐漸增加色彩損失的權重\n",
    "        color_weight = min(1.0, 0.2 + epoch * 0.02)\n",
    "        loss = main_loss + color_weight * col_loss\n",
    "        \n",
    "        # 反向傳播更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪防止爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += main_loss.item()\n",
    "        color_loss_total += col_loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_color_loss = color_loss_total / len(loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.5f}, Color Loss: {avg_color_loss:.5f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    return avg_loss, avg_color_loss\n",
    "\n",
    "# 驗證函數\n",
    "def validate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    color_loss_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x0, _ in tqdm(loader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            x0 = x0.to(device)\n",
    "            b = x0.size(0)\n",
    "            \n",
    "            # 固定質量用於評估\n",
    "            quality = torch.randint(10, 90, (b,)).cpu().numpy()\n",
    "            xt = torch.stack([jpeg_compress(x0[i:i+1], int(q)) for i, q in enumerate(quality)]).squeeze()\n",
    "            \n",
    "            # 使用中間時間步進行評估\n",
    "            t = torch.full((b,), num_timesteps//2, device=device).long()\n",
    "            \n",
    "            # 計算噪聲\n",
    "            noise = x0 - xt\n",
    "            \n",
    "            # 模型預測噪聲\n",
    "            pred_noise = model(xt, t.float()/num_timesteps)\n",
    "            \n",
    "            # 計算損失\n",
    "            main_loss = loss_fn(pred_noise, noise)\n",
    "            col_loss = color_loss(x0, xt + pred_noise)\n",
    "            \n",
    "            total_loss += main_loss.item()\n",
    "            color_loss_total += col_loss.item()\n",
    "            \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_color_loss = color_loss_total / len(loader)\n",
    "    print(f\"Validation - Avg Loss: {avg_loss:.5f}, Color Loss: {avg_color_loss:.5f}\")\n",
    "    \n",
    "    # 保存一些復原效果樣本用於視覺檢查\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            x0, _ = next(iter(test_dataloader))\n",
    "            x0 = x0.to(device)\n",
    "            \n",
    "            # 選擇不同的質量級別進行可視化\n",
    "            qualities = [10, 30, 50, 70]\n",
    "            plt.figure(figsize=(len(qualities)*3+3, 4))\n",
    "            \n",
    "            # 顯示原始圖像\n",
    "            plt.subplot(1, len(qualities)+1, 1)\n",
    "            plt.imshow(x0[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "            plt.title(\"Original\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # 顯示不同質量的JPEG壓縮和復原效果\n",
    "            for i, q in enumerate(qualities):\n",
    "                xt = jpeg_compress(x0, q)\n",
    "                \n",
    "                # 設定初始時間步基於質量\n",
    "                t_step = int((100 - q) / 100 * num_timesteps)\n",
    "                t_tensor = torch.full((1,), t_step, device=device).float() / num_timesteps\n",
    "                \n",
    "                # 預測噪聲\n",
    "                pred_noise = model(xt, t_tensor)\n",
    "                restored = xt + pred_noise\n",
    "                \n",
    "                plt.subplot(1, len(qualities)+1, i+2)\n",
    "                plt.imshow(torch.cat([\n",
    "                    xt[0].cpu().permute(1,2,0),\n",
    "                    restored[0].cpu().permute(1,2,0)\n",
    "                ], dim=1)*0.5+0.5)\n",
    "                plt.title(f\"Q{q}: JPEG vs Restored\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'validation_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    return avg_loss, avg_color_loss\n",
    "\n",
    "# 測試復原效果，使用高斯混合模型採樣器\n",
    "def test_restoration(model, quality_levels=[10, 30, 50, 70]):\n",
    "    # 初始化採樣器\n",
    "    gmm_sampler = GaussianMixtureSampler(model)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0, _ = next(iter(test_dataloader))\n",
    "        x0 = x0.to(device)\n",
    "        \n",
    "        plt.figure(figsize=(len(quality_levels)*3+3, 6))\n",
    "        \n",
    "        # 顯示原始圖像\n",
    "        plt.subplot(2, len(quality_levels)+1, 1)\n",
    "        plt.imshow(x0[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 為每個質量級別顯示JPEG和復原效果\n",
    "        for i, q in enumerate(quality_levels):\n",
    "            # JPEG壓縮\n",
    "            xt = jpeg_compress(x0, q)\n",
    "            \n",
    "            # 設定初始時間步基於質量\n",
    "            init_t = int((100 - q) / 100 * num_timesteps)\n",
    "            \n",
    "            # 使用GMM採樣器進行復原\n",
    "            restored_gmm = gmm_sampler.sample(xt, steps=init_t+1)\n",
    "            \n",
    "            # 顯示JPEG壓縮結果\n",
    "            plt.subplot(2, len(quality_levels)+1, i+2)\n",
    "            plt.imshow(xt[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "            plt.title(f\"JPEG Q{q}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # 顯示復原結果\n",
    "            plt.subplot(2, len(quality_levels)+1, len(quality_levels)+i+2)\n",
    "            plt.imshow(restored_gmm[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "            plt.title(f\"Restored Q{q}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('final_restoration_results.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # 計算和顯示復原指標\n",
    "        metrics = {}\n",
    "        for q in quality_levels:\n",
    "            xt = jpeg_compress(x0, q)\n",
    "            init_t = int((100 - q) / 100 * num_timesteps)\n",
    "            restored_gmm = gmm_sampler.sample(xt, steps=init_t+1)\n",
    "            \n",
    "            # 計算PSNR (原始圖像與復原圖像)\n",
    "            mse = F.mse_loss(restored_gmm, x0).item()\n",
    "            psnr = 10 * math.log10(1.0 / mse)\n",
    "            \n",
    "            # 計算色彩保留指標\n",
    "            col_loss = color_loss(restored_gmm, x0).item()\n",
    "            \n",
    "            metrics[q] = {\n",
    "                'PSNR': psnr,\n",
    "                'Color_Loss': col_loss\n",
    "            }\n",
    "        \n",
    "        # 顯示指標\n",
    "        print(\"\\nRestoration Quality Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Quality':<10}{'PSNR (dB)':<15}{'Color Loss':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        for q in quality_levels:\n",
    "            print(f\"{q:<10}{metrics[q]['PSNR']:<15.2f}{metrics[q]['Color_Loss']:<15.5f}\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# 訓練循環\n",
    "def train_model(epochs=100, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    color_losses = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 訓練一個周期\n",
    "        train_loss, train_color_loss = train_epoch(model, train_dataloader, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 驗證一個周期\n",
    "        val_loss, val_color_loss = validate(model, valid_dataloader, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "        color_losses.append(val_color_loss)\n",
    "        \n",
    "        # 更新學習率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 檢查是否需要保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'color_loss': val_color_loss\n",
    "            }, f\"best_jpeg_diffusion.pth\")\n",
    "            print(f\"New best model saved with val loss {val_loss:.5f} and color loss {val_color_loss:.5f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs!\")\n",
    "                break\n",
    "        \n",
    "        # 繪製訓練曲線\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(color_losses, label='Color Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Color Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # 載入最佳模型\n",
    "    checkpoint = torch.load(\"best_jpeg_diffusion.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.5f}\")\n",
    "    \n",
    "    # 測試復原效果\n",
    "    test_restoration(model)\n",
    "\n",
    "# 執行訓練\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "總參數數量: 77,836,105\n",
      "成功載入模型權重，來自 epoch 1\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 419.26it/s]\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 22294 (\\N{CJK UNIFIED IDEOGRAPH-5716}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 20687 (\\N{CJK UNIFIED IDEOGRAPH-50CF}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:344: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 22294 (\\N{CJK UNIFIED IDEOGRAPH-5716}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 20687 (\\N{CJK UNIFIED IDEOGRAPH-50CF}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:345: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 1, 質量 10: PSNR (壓縮/恢復): 22.53dB/6.36dB, SSIM (壓縮/恢復): 0.6776/0.0291, LPIPS (壓縮/恢復): 0.0395/0.2888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 362.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 1, 質量 30: PSNR (壓縮/恢復): 26.93dB/7.70dB, SSIM (壓縮/恢復): 0.8262/0.0865, LPIPS (壓縮/恢復): 0.0055/0.2074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 354.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 1, 質量 50: PSNR (壓縮/恢復): 29.85dB/11.54dB, SSIM (壓縮/恢復): 0.8872/0.2402, LPIPS (壓縮/恢復): 0.0019/0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 356.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 1, 質量 70: PSNR (壓縮/恢復): 31.64dB/18.95dB, SSIM (壓縮/恢復): 0.9300/0.5584, LPIPS (壓縮/恢復): 0.0008/0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 360.54it/s]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 367.70it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 363.65it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 368.54it/s]\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 22294 (\\N{CJK UNIFIED IDEOGRAPH-5716}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 20687 (\\N{CJK UNIFIED IDEOGRAPH-50CF}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 24478 (\\N{CJK UNIFIED IDEOGRAPH-5F9E}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:378: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 22294 (\\N{CJK UNIFIED IDEOGRAPH-5716}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 20687 (\\N{CJK UNIFIED IDEOGRAPH-50CF}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 24478 (\\N{CJK UNIFIED IDEOGRAPH-5F9E}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:379: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 428.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 2, 質量 10: PSNR (壓縮/恢復): 22.89dB/6.26dB, SSIM (壓縮/恢復): 0.8138/0.0307, LPIPS (壓縮/恢復): 0.0452/0.3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 429.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 2, 質量 30: PSNR (壓縮/恢復): 26.49dB/8.01dB, SSIM (壓縮/恢復): 0.9165/0.1366, LPIPS (壓縮/恢復): 0.0112/0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 432.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 2, 質量 50: PSNR (壓縮/恢復): 28.64dB/11.73dB, SSIM (壓縮/恢復): 0.9451/0.3041, LPIPS (壓縮/恢復): 0.0061/0.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 431.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 2, 質量 70: PSNR (壓縮/恢復): 31.36dB/19.04dB, SSIM (壓縮/恢復): 0.9686/0.6624, LPIPS (壓縮/恢復): 0.0034/0.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 433.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 3, 質量 10: PSNR (壓縮/恢復): 23.61dB/6.20dB, SSIM (壓縮/恢復): 0.7235/0.0210, LPIPS (壓縮/恢復): 0.0471/0.3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 433.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 3, 質量 30: PSNR (壓縮/恢復): 26.77dB/7.25dB, SSIM (壓縮/恢復): 0.8496/0.0954, LPIPS (壓縮/恢復): 0.0135/0.3255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 429.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 3, 質量 50: PSNR (壓縮/恢復): 28.80dB/11.27dB, SSIM (壓縮/恢復): 0.8959/0.2147, LPIPS (壓縮/恢復): 0.0037/0.2663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 431.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 3, 質量 70: PSNR (壓縮/恢復): 30.67dB/20.28dB, SSIM (壓縮/恢復): 0.9363/0.5318, LPIPS (壓縮/恢復): 0.0020/0.1921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 434.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 4, 質量 10: PSNR (壓縮/恢復): 24.80dB/5.62dB, SSIM (壓縮/恢復): 0.7262/0.0304, LPIPS (壓縮/恢復): 0.0418/0.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 435.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 4, 質量 30: PSNR (壓縮/恢復): 27.91dB/6.44dB, SSIM (壓縮/恢復): 0.8208/0.0567, LPIPS (壓縮/恢復): 0.0134/0.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 429.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 4, 質量 50: PSNR (壓縮/恢復): 30.96dB/10.73dB, SSIM (壓縮/恢復): 0.8897/0.2061, LPIPS (壓縮/恢復): 0.0046/0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 431.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 4, 質量 70: PSNR (壓縮/恢復): 32.72dB/19.56dB, SSIM (壓縮/恢復): 0.9279/0.4995, LPIPS (壓縮/恢復): 0.0016/0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 434.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 5, 質量 10: PSNR (壓縮/恢復): 25.80dB/6.04dB, SSIM (壓縮/恢復): 0.6667/0.0189, LPIPS (壓縮/恢復): 0.0103/0.3973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 434.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 5, 質量 30: PSNR (壓縮/恢復): 29.02dB/7.56dB, SSIM (壓縮/恢復): 0.8182/0.1012, LPIPS (壓縮/恢復): 0.0044/0.3782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 427.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 5, 質量 50: PSNR (壓縮/恢復): 31.72dB/11.32dB, SSIM (壓縮/恢復): 0.8822/0.2394, LPIPS (壓縮/恢復): 0.0021/0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 429.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 5, 質量 70: PSNR (壓縮/恢復): 33.55dB/17.81dB, SSIM (壓縮/恢復): 0.9193/0.5280, LPIPS (壓縮/恢復): 0.0007/0.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 434.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 6, 質量 10: PSNR (壓縮/恢復): 22.71dB/6.05dB, SSIM (壓縮/恢復): 0.7813/0.0187, LPIPS (壓縮/恢復): 0.0310/0.2648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 437.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 6, 質量 30: PSNR (壓縮/恢復): 25.82dB/8.37dB, SSIM (壓縮/恢復): 0.8778/0.0869, LPIPS (壓縮/恢復): 0.0092/0.2484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 436.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 6, 質量 50: PSNR (壓縮/恢復): 28.54dB/11.87dB, SSIM (壓縮/恢復): 0.9190/0.2514, LPIPS (壓縮/恢復): 0.0057/0.1847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 428.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 6, 質量 70: PSNR (壓縮/恢復): 30.38dB/18.30dB, SSIM (壓縮/恢復): 0.9416/0.5135, LPIPS (壓縮/恢復): 0.0018/0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 433.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 7, 質量 10: PSNR (壓縮/恢復): 24.71dB/6.45dB, SSIM (壓縮/恢復): 0.7879/0.0302, LPIPS (壓縮/恢復): 0.0962/0.2588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 434.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 7, 質量 30: PSNR (壓縮/恢復): 28.11dB/8.36dB, SSIM (壓縮/恢復): 0.9015/0.1175, LPIPS (壓縮/恢復): 0.0488/0.2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 431.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 7, 質量 50: PSNR (壓縮/恢復): 30.95dB/12.18dB, SSIM (壓縮/恢復): 0.9380/0.2933, LPIPS (壓縮/恢復): 0.0090/0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 433.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 7, 質量 70: PSNR (壓縮/恢復): 32.62dB/19.44dB, SSIM (壓縮/恢復): 0.9576/0.6361, LPIPS (壓縮/恢復): 0.0026/0.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 426.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 8, 質量 10: PSNR (壓縮/恢復): 23.39dB/6.43dB, SSIM (壓縮/恢復): 0.7690/0.0296, LPIPS (壓縮/恢復): 0.0482/0.2687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 433.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 8, 質量 30: PSNR (壓縮/恢復): 26.71dB/8.15dB, SSIM (壓縮/恢復): 0.8731/0.1204, LPIPS (壓縮/恢復): 0.0278/0.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 432.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 8, 質量 50: PSNR (壓縮/恢復): 29.22dB/11.76dB, SSIM (壓縮/恢復): 0.9212/0.2894, LPIPS (壓縮/恢復): 0.0068/0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 433.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 8, 質量 70: PSNR (壓縮/恢復): 30.89dB/19.67dB, SSIM (壓縮/恢復): 0.9465/0.5920, LPIPS (壓縮/恢復): 0.0025/0.1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 431.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 9, 質量 10: PSNR (壓縮/恢復): 29.65dB/6.65dB, SSIM (壓縮/恢復): 0.7884/0.0095, LPIPS (壓縮/恢復): 0.0812/0.4427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 434.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 9, 質量 30: PSNR (壓縮/恢復): 34.36dB/8.43dB, SSIM (壓縮/恢復): 0.8874/0.0239, LPIPS (壓縮/恢復): 0.0179/0.4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 429.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 9, 質量 50: PSNR (壓縮/恢復): 36.48dB/12.39dB, SSIM (壓縮/恢復): 0.9161/0.1235, LPIPS (壓縮/恢復): 0.0125/0.3731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 433.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 9, 質量 70: PSNR (壓縮/恢復): 37.65dB/21.99dB, SSIM (壓縮/恢復): 0.9348/0.3725, LPIPS (壓縮/恢復): 0.0111/0.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 434.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 10, 質量 10: PSNR (壓縮/恢復): 20.23dB/5.70dB, SSIM (壓縮/恢復): 0.6995/0.0312, LPIPS (壓縮/恢復): 0.0393/0.3059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 433.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 10, 質量 30: PSNR (壓縮/恢復): 23.15dB/7.42dB, SSIM (壓縮/恢復): 0.8266/0.0942, LPIPS (壓縮/恢復): 0.0223/0.2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 428.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 10, 質量 50: PSNR (壓縮/恢復): 26.80dB/10.76dB, SSIM (壓縮/恢復): 0.9092/0.2485, LPIPS (壓縮/恢復): 0.0019/0.1665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 431.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像 10, 質量 70: PSNR (壓縮/恢復): 28.82dB/16.62dB, SSIM (壓縮/恢復): 0.9360/0.5603, LPIPS (壓縮/恢復): 0.0006/0.0960\n",
      "\n",
      "JPEG 質量: 10\n",
      "平均 PSNR (原始 vs 壓縮): 24.03 dB\n",
      "平均 PSNR (原始 vs 恢復): 6.18 dB\n",
      "PSNR 提升: -17.86 dB\n",
      "平均 SSIM (原始 vs 壓縮): 0.7434\n",
      "平均 SSIM (原始 vs 恢復): 0.0249\n",
      "SSIM 提升: -0.7185\n",
      "平均 LPIPS (原始 vs 壓縮): 0.0480\n",
      "平均 LPIPS (原始 vs 恢復): 0.3142\n",
      "LPIPS 改善: -0.2663\n",
      "\n",
      "JPEG 質量: 30\n",
      "平均 PSNR (原始 vs 壓縮): 27.53 dB\n",
      "平均 PSNR (原始 vs 恢復): 7.77 dB\n",
      "PSNR 提升: -19.76 dB\n",
      "平均 SSIM (原始 vs 壓縮): 0.8598\n",
      "平均 SSIM (原始 vs 恢復): 0.0919\n",
      "SSIM 提升: -0.7678\n",
      "平均 LPIPS (原始 vs 壓縮): 0.0174\n",
      "平均 LPIPS (原始 vs 恢復): 0.3009\n",
      "LPIPS 改善: -0.2835\n",
      "\n",
      "JPEG 質量: 50\n",
      "平均 PSNR (原始 vs 壓縮): 30.20 dB\n",
      "平均 PSNR (原始 vs 恢復): 11.55 dB\n",
      "PSNR 提升: -18.64 dB\n",
      "平均 SSIM (原始 vs 壓縮): 0.9104\n",
      "平均 SSIM (原始 vs 恢復): 0.2411\n",
      "SSIM 提升: -0.6693\n",
      "平均 LPIPS (原始 vs 壓縮): 0.0054\n",
      "平均 LPIPS (原始 vs 恢復): 0.2205\n",
      "LPIPS 改善: -0.2151\n",
      "\n",
      "JPEG 質量: 70\n",
      "平均 PSNR (原始 vs 壓縮): 32.03 dB\n",
      "平均 PSNR (原始 vs 恢復): 19.16 dB\n",
      "PSNR 提升: -12.86 dB\n",
      "平均 SSIM (原始 vs 壓縮): 0.9399\n",
      "平均 SSIM (原始 vs 恢復): 0.5455\n",
      "SSIM 提升: -0.3944\n",
      "平均 LPIPS (原始 vs 壓縮): 0.0027\n",
      "平均 LPIPS (原始 vs 恢復): 0.1460\n",
      "LPIPS 改善: -0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 36074 (\\N{CJK UNIFIED IDEOGRAPH-8CEA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 36611 (\\N{CJK UNIFIED IDEOGRAPH-8F03}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 22739 (\\N{CJK UNIFIED IDEOGRAPH-58D3}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 32302 (\\N{CJK UNIFIED IDEOGRAPH-7E2E}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 36234 (\\N{CJK UNIFIED IDEOGRAPH-8D8A}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:443: UserWarning: Glyph 22909 (\\N{CJK UNIFIED IDEOGRAPH-597D}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 36611 (\\N{CJK UNIFIED IDEOGRAPH-8F03}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 36074 (\\N{CJK UNIFIED IDEOGRAPH-8CEA}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 22739 (\\N{CJK UNIFIED IDEOGRAPH-58D3}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 32302 (\\N{CJK UNIFIED IDEOGRAPH-7E2E}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 24674 (\\N{CJK UNIFIED IDEOGRAPH-6062}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 24489 (\\N{CJK UNIFIED IDEOGRAPH-5FA9}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 36234 (\\N{CJK UNIFIED IDEOGRAPH-8D8A}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
      "/tmp/ipykernel_3062/1484099668.py:444: UserWarning: Glyph 22909 (\\N{CJK UNIFIED IDEOGRAPH-597D}) missing from current font.\n",
      "  plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評估完成，結果已保存至 'inference_results' 目錄\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pytorch_msssim import ssim\n",
    "import lpips\n",
    "\n",
    "# 設備設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 資料預處理\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 下載 CIFAR-10 測試資料集\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# JPEG 壓縮函數\n",
    "def jpeg_compress(x, quality):\n",
    "    \"\"\"執行JPEG壓縮並提升色彩保存效果\"\"\"\n",
    "    x = (x * 127.5 + 127.5).clamp(0, 255).to(torch.uint8).cpu()\n",
    "    compressed_images = []\n",
    "    for img in x:\n",
    "        pil_img = torchvision.transforms.ToPILImage()(img)\n",
    "        buffer = io.BytesIO()\n",
    "        # 使用4:4:4色度採樣保持更好的色彩信息，除非質量非常低\n",
    "        subsampling = \"4:4:4\" if quality > 30 else \"4:2:0\"\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=quality, subsampling=subsampling)\n",
    "        buffer.seek(0)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        compressed_tensor = torchvision.transforms.ToTensor()(compressed_img)\n",
    "        compressed_images.append(compressed_tensor)\n",
    "    return torch.stack(compressed_images).to(device).sub(0.5).div(0.5)\n",
    "\n",
    "# 時間嵌入模組\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.proj(emb)\n",
    "\n",
    "# 殘差注意力模塊\n",
    "class ResAttnBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 確保組數能被通道數整除\n",
    "        num_groups = min(8, in_c)\n",
    "        while in_c % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "            \n",
    "        self.norm1 = nn.GroupNorm(num_groups, in_c)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.time_proj = nn.Linear(time_dim, out_c)\n",
    "        \n",
    "        # 調整 out_c 的組數\n",
    "        num_groups_out = min(8, out_c)\n",
    "        while out_c % num_groups_out != 0 and num_groups_out > 1:\n",
    "            num_groups_out -= 1\n",
    "            \n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.attn = nn.MultiheadAttention(out_c, 4, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        # 加入時間編碼\n",
    "        t = self.time_proj(t_emb)[..., None, None]\n",
    "        h = h + t\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = self.conv2(F.silu(h))\n",
    "        \n",
    "        # 應用自注意力機制\n",
    "        b, c, hh, ww = h.shape\n",
    "        h_attn = h.view(b, c, -1).permute(0, 2, 1)\n",
    "        h_attn, _ = self.attn(h_attn, h_attn, h_attn)\n",
    "        h_attn = h_attn.permute(0, 2, 1).view(b, c, hh, ww)\n",
    "        \n",
    "        return self.shortcut(x) + self.dropout(h_attn)\n",
    "\n",
    "# 改進的UNet架構\n",
    "class JPEGDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        time_dim = 256\n",
    "        self.time_embed = TimeEmbedding(time_dim)\n",
    "        \n",
    "        # 下采样路径 - 增强通道数以提高色彩保存能力\n",
    "        self.down1 = ResAttnBlock(3, 64, time_dim)\n",
    "        self.down2 = ResAttnBlock(64, 128, time_dim)\n",
    "        self.down3 = ResAttnBlock(128, 256, time_dim)\n",
    "        self.down4 = ResAttnBlock(256, 512, time_dim)\n",
    "        self.down5 = ResAttnBlock(512, 512, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 瓶颈层\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResAttnBlock(512, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 512, time_dim)\n",
    "        )\n",
    "        \n",
    "        # 上采样路径\n",
    "        self.up1 = ResAttnBlock(1024, 512, time_dim)\n",
    "        self.up2 = ResAttnBlock(512 + 512, 256, time_dim)\n",
    "        self.up3 = ResAttnBlock(256 + 256, 128, time_dim)\n",
    "        self.up4 = ResAttnBlock(128 + 128, 64, time_dim)\n",
    "        self.up5 = ResAttnBlock(64 + 64, 64, time_dim)\n",
    "        \n",
    "        # 输出层 - 使用1x1卷积保留空间色彩相关性\n",
    "        self.out_conv = nn.Conv2d(64, 3, 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # 下采样\n",
    "        d1 = self.down1(x, t_emb)  # 32x32\n",
    "        d2 = self.down2(self.pool(d1), t_emb)  # 16x16\n",
    "        d3 = self.down3(self.pool(d2), t_emb)  # 8x8\n",
    "        d4 = self.down4(self.pool(d3), t_emb)  # 4x4\n",
    "        d5 = self.down5(self.pool(d4), t_emb)  # 2x2\n",
    "        \n",
    "        # 瓶颈层\n",
    "        b = self.bottleneck[0](self.pool(d5), t_emb)\n",
    "        b = self.bottleneck[1](b, t_emb)\n",
    "        b = self.bottleneck[2](b, t_emb)\n",
    "        \n",
    "        # 上采样 - 使用双线性上采样避免棋盘格效应\n",
    "        u1 = self.up1(torch.cat([F.interpolate(b, scale_factor=2, mode='bilinear', align_corners=False), d5], dim=1), t_emb)\n",
    "        u2 = self.up2(torch.cat([F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False), d4], dim=1), t_emb)\n",
    "        u3 = self.up3(torch.cat([F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False), d3], dim=1), t_emb)\n",
    "        u4 = self.up4(torch.cat([F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False), d2], dim=1), t_emb)\n",
    "        u5 = self.up5(torch.cat([F.interpolate(u4, scale_factor=2, mode='bilinear', align_corners=False), d1], dim=1), t_emb)\n",
    "        \n",
    "        return self.out_conv(u5)\n",
    "\n",
    "# 初始化模型\n",
    "model = JPEGDiffusionModel().to(device)\n",
    "print(f\"總參數數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 高斯混合模型採樣器\n",
    "class GaussianMixtureSampler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def optimize_mixture_params(self, x_t, pred_noise, t_step, t_next):\n",
    "        \"\"\"優化高斯混合模型參數\"\"\"\n",
    "        # 預測的x0\n",
    "        x0_pred = x_t + pred_noise\n",
    "        \n",
    "        # 使用預測的x0計算雙峰高斯分布的均值\n",
    "        # 第一個均值 - 更傾向於保持原始預測\n",
    "        mu1 = x0_pred * 0.9 + x_t * 0.1\n",
    "        \n",
    "        # 第二個均值 - 更傾向於向原始圖像方向移動\n",
    "        mu2 = x0_pred * 1.1 - x_t * 0.1\n",
    "        \n",
    "        # 估算標準差 - 隨時間變化\n",
    "        # 接近結束時使用較小的標準差\n",
    "        time_weight = t_step / num_timesteps\n",
    "        sigma_base = 0.15 * time_weight\n",
    "        \n",
    "        return mu1, mu2, sigma_base\n",
    "        \n",
    "    def sample(self, x_t, steps=100, guidance_scale=1.0):\n",
    "        \"\"\"使用高斯混合模型進行採樣\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 從給定的噪聲圖像開始\n",
    "            for i in tqdm(range(steps-1, -1, -1), desc=\"Sampling\"):\n",
    "                t = torch.full((x_t.size(0),), i, device=device).float() / num_timesteps\n",
    "                \n",
    "                # 獲取噪聲預測\n",
    "                pred_noise = self.model(x_t, t)\n",
    "                \n",
    "                if i > 0:\n",
    "                    # 計算高斯混合模型參數\n",
    "                    mu1, mu2, sigma = self.optimize_mixture_params(x_t, pred_noise, i, i-1)\n",
    "                    \n",
    "                    # 隨機選擇使用哪個高斯分量\n",
    "                    if random.random() < 0.33:  # 1/3概率使用第一個均值\n",
    "                        next_mean = mu1\n",
    "                    else:  # 2/3概率使用第二個均值\n",
    "                        next_mean = mu2\n",
    "                    \n",
    "                    # 隨機性逐漸減少，接近原始圖像時幾乎無隨機性\n",
    "                    noise_scale = sigma * (1.0 - (steps - i) / steps) * guidance_scale\n",
    "                    \n",
    "                    # 下一步\n",
    "                    x_t = next_mean + torch.randn_like(x_t) * noise_scale\n",
    "                else:\n",
    "                    # 最後一步直接使用預測的原始圖像\n",
    "                    x_t = x_t + pred_noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "# 設定JPEG擴散參數\n",
    "num_timesteps = 100\n",
    "# 根據論文，使用余弦調度的噪聲\n",
    "betas = torch.linspace(1e-4, 0.02, num_timesteps).to(device)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# 嘗試加載模型權重\n",
    "try:\n",
    "    checkpoint = torch.load(\"best_jpeg_diffusion.pth\")\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"成功載入模型權重，來自 epoch {checkpoint['epoch']+1}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"成功載入模型權重!\")\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"載入模型失敗: {e}\")\n",
    "    print(\"請確保 'best_jpeg_diffusion.pth' 文件存在且有效\")\n",
    "    exit()\n",
    "\n",
    "# 初始化採樣器\n",
    "sampler = GaussianMixtureSampler(model)\n",
    "\n",
    "# 初始化LPIPS函數\n",
    "lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# 計算PSNR\n",
    "def calculate_psnr(img1, img2):\n",
    "    # 確保值域在[0, 1]之間\n",
    "    img1 = (img1 * 0.5 + 0.5).clamp(0, 1)\n",
    "    img2 = (img2 * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 10 * torch.log10(1.0 / mse)\n",
    "\n",
    "# 創建保存結果的資料夾\n",
    "os.makedirs(\"inference_results\", exist_ok=True)\n",
    "\n",
    "# 運行推理與評估\n",
    "def run_inference(num_samples=10, qualities=[10, 30, 50, 70]):\n",
    "    \"\"\"執行推理與評估\"\"\"\n",
    "    psnr_compressed_total = {q: 0.0 for q in qualities}\n",
    "    psnr_restored_total = {q: 0.0 for q in qualities}\n",
    "    ssim_compressed_total = {q: 0.0 for q in qualities}\n",
    "    ssim_restored_total = {q: 0.0 for q in qualities}\n",
    "    lpips_compressed_total = {q: 0.0 for q in qualities}\n",
    "    lpips_restored_total = {q: 0.0 for q in qualities}\n",
    "    \n",
    "    for i, (x0, _) in enumerate(test_dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        \n",
    "        x0 = x0.to(device)\n",
    "        \n",
    "        for quality in qualities:\n",
    "            # JPEG壓縮\n",
    "            x_compressed = jpeg_compress(x0, quality)\n",
    "            \n",
    "            # 設定初始時間步基於質量\n",
    "            init_t = int((100 - quality) / 100 * num_timesteps)\n",
    "            \n",
    "            # 使用高斯混合模型採樣器進行復原\n",
    "            x_restored = sampler.sample(x_compressed, steps=init_t+1, guidance_scale=0.8)\n",
    "            \n",
    "            # 轉換到[0,1]範圍用於計算指標\n",
    "            x0_01 = (x0 * 0.5 + 0.5).clamp(0, 1)\n",
    "            x_compressed_01 = (x_compressed * 0.5 + 0.5).clamp(0, 1)\n",
    "            x_restored_01 = (x_restored * 0.5 + 0.5).clamp(0, 1)\n",
    "            \n",
    "            # 計算PSNR\n",
    "            psnr_comp = calculate_psnr(x0, x_compressed).item()\n",
    "            psnr_rest = calculate_psnr(x0, x_restored).item()\n",
    "            \n",
    "            # 計算SSIM\n",
    "            ssim_comp = ssim(x0_01, x_compressed_01, data_range=1.0).item()\n",
    "            ssim_rest = ssim(x0_01, x_restored_01, data_range=1.0).item()\n",
    "            \n",
    "            # 計算LPIPS (較低的值表示更好的感知相似度)\n",
    "            lpips_comp = lpips_fn(x0 * 2, x_compressed * 2).item()  # LPIPS期望[-1,1]範圍\n",
    "            lpips_rest = lpips_fn(x0 * 2, x_restored * 2).item()\n",
    "            \n",
    "            # 累計指標\n",
    "            psnr_compressed_total[quality] += psnr_comp\n",
    "            psnr_restored_total[quality] += psnr_rest\n",
    "            ssim_compressed_total[quality] += ssim_comp\n",
    "            ssim_restored_total[quality] += ssim_rest\n",
    "            lpips_compressed_total[quality] += lpips_comp\n",
    "            lpips_restored_total[quality] += lpips_rest\n",
    "            \n",
    "            print(f\"影像 {i+1}, 質量 {quality}: \"\n",
    "                  f\"PSNR (壓縮/恢復): {psnr_comp:.2f}dB/{psnr_rest:.2f}dB, \"\n",
    "                  f\"SSIM (壓縮/恢復): {ssim_comp:.4f}/{ssim_rest:.4f}, \"\n",
    "                  f\"LPIPS (壓縮/恢復): {lpips_comp:.4f}/{lpips_rest:.4f}\")\n",
    "            \n",
    "            # 每個質量級別的第一張圖片儲存用於視覺比較\n",
    "            if i == 0:\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                axes[0].imshow(x0_01[0].cpu().permute(1, 2, 0))\n",
    "                axes[0].set_title(\"原始圖像\")\n",
    "                axes[0].axis(\"off\")\n",
    "                \n",
    "                axes[1].imshow(x_compressed_01[0].cpu().permute(1, 2, 0))\n",
    "                axes[1].set_title(f\"JPEG Q{quality}\\nPSNR: {psnr_comp:.2f}dB\\nSSIM: {ssim_comp:.4f}\\nLPIPS: {lpips_comp:.4f}\")\n",
    "                axes[1].axis(\"off\")\n",
    "                \n",
    "                axes[2].imshow(x_restored_01[0].cpu().permute(1, 2, 0))\n",
    "                axes[2].set_title(f\"恢復圖像\\nPSNR: {psnr_rest:.2f}dB\\nSSIM: {ssim_rest:.4f}\\nLPIPS: {lpips_rest:.4f}\")\n",
    "                axes[2].axis(\"off\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"inference_results/quality_{quality}_comparison.png\", dpi=150)\n",
    "                plt.close()\n",
    "                \n",
    "        # 顯示一個完整的例子(所有質量級別)\n",
    "        if i == 0:\n",
    "            n_cols = len(qualities) + 1\n",
    "            fig, axes = plt.subplots(2, n_cols, figsize=(n_cols*4, 8))\n",
    "            \n",
    "            # 第一行顯示原圖和各種品質的壓縮圖\n",
    "            axes[0, 0].imshow((x0[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "            axes[0, 0].set_title(\"原始圖像\")\n",
    "            axes[0, 0].axis(\"off\")\n",
    "            \n",
    "            for j, quality in enumerate(qualities):\n",
    "                x_compressed = jpeg_compress(x0, quality)\n",
    "                axes[0, j+1].imshow((x_compressed[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "                axes[0, j+1].set_title(f\"JPEG Q{quality}\")\n",
    "                axes[0, j+1].axis(\"off\")\n",
    "            \n",
    "            # 第二行顯示原圖和各種品質的復原圖\n",
    "            axes[1, 0].imshow((x0[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "            axes[1, 0].set_title(\"原始圖像\")\n",
    "            axes[1, 0].axis(\"off\")\n",
    "            \n",
    "            for j, quality in enumerate(qualities):\n",
    "                x_compressed = jpeg_compress(x0, quality)\n",
    "                init_t = int((100 - quality) / 100 * num_timesteps)\n",
    "                x_restored = sampler.sample(x_compressed, steps=init_t+1)\n",
    "                \n",
    "                axes[1, j+1].imshow((x_restored[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "                axes[1, j+1].set_title(f\"從Q{quality}恢復\")\n",
    "                axes[1, j+1].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"inference_results/all_qualities_comparison.png\", dpi=150)\n",
    "            plt.close()\n",
    "    \n",
    "    # 計算平均指標\n",
    "    for quality in qualities:\n",
    "        avg_psnr_comp = psnr_compressed_total[quality] / num_samples\n",
    "        avg_psnr_rest = psnr_restored_total[quality] / num_samples\n",
    "        avg_ssim_comp = ssim_compressed_total[quality] / num_samples\n",
    "        avg_ssim_rest = ssim_restored_total[quality] / num_samples\n",
    "        avg_lpips_comp = lpips_compressed_total[quality] / num_samples\n",
    "        avg_lpips_rest = lpips_restored_total[quality] / num_samples\n",
    "        \n",
    "        psnr_gain = avg_psnr_rest - avg_psnr_comp\n",
    "        ssim_gain = avg_ssim_rest - avg_ssim_comp\n",
    "        lpips_improvement = avg_lpips_comp - avg_lpips_rest  # LPIPS越低越好\n",
    "        \n",
    "        print(f\"\\nJPEG 質量: {quality}\")\n",
    "        print(f\"平均 PSNR (原始 vs 壓縮): {avg_psnr_comp:.2f} dB\")\n",
    "        print(f\"平均 PSNR (原始 vs 恢復): {avg_psnr_rest:.2f} dB\")\n",
    "        print(f\"PSNR 提升: {psnr_gain:.2f} dB\")\n",
    "        \n",
    "        print(f\"平均 SSIM (原始 vs 壓縮): {avg_ssim_comp:.4f}\")\n",
    "        print(f\"平均 SSIM (原始 vs 恢復): {avg_ssim_rest:.4f}\")\n",
    "        print(f\"SSIM 提升: {ssim_gain:.4f}\")\n",
    "        \n",
    "        print(f\"平均 LPIPS (原始 vs 壓縮): {avg_lpips_comp:.4f}\")\n",
    "        print(f\"平均 LPIPS (原始 vs 恢復): {avg_lpips_rest:.4f}\")\n",
    "        print(f\"LPIPS 改善: {lpips_improvement:.4f}\")\n",
    "    \n",
    "    # 繪製比較圖\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # PSNR比較圖\n",
    "    plt.subplot(1, 3, 1)\n",
    "    x = np.arange(len(qualities))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, [psnr_compressed_total[q]/num_samples for q in qualities], width, label='壓縮')\n",
    "    plt.bar(x + width/2, [psnr_restored_total[q]/num_samples for q in qualities], width, label='恢復')\n",
    "    plt.xlabel('JPEG 質量')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.title('PSNR 比較')\n",
    "    plt.xticks(x, qualities)\n",
    "    plt.legend()\n",
    "    \n",
    "    # SSIM比較圖\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(x - width/2, [ssim_compressed_total[q]/num_samples for q in qualities], width, label='壓縮')\n",
    "    plt.bar(x + width/2, [ssim_restored_total[q]/num_samples for q in qualities], width, label='恢復')\n",
    "    plt.xlabel('JPEG 質量')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.title('SSIM 比較')\n",
    "    plt.xticks(x, qualities)\n",
    "    plt.legend()\n",
    "    \n",
    "    # LPIPS比較圖\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(x - width/2, [lpips_compressed_total[q]/num_samples for q in qualities], width, label='壓縮')\n",
    "    plt.bar(x + width/2, [lpips_restored_total[q]/num_samples for q in qualities], width, label='恢復')\n",
    "    plt.xlabel('JPEG 質量')\n",
    "    plt.ylabel('LPIPS (越低越好)')\n",
    "    plt.title('LPIPS 比較')\n",
    "    plt.xticks(x, qualities)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"inference_results/metrics_comparison.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n評估完成，結果已保存至 'inference_results' 目錄\")\n",
    "\n",
    "# 主函數\n",
    "if __name__ == \"__main__\":\n",
    "    # 設定要測試的樣本數量\n",
    "    num_samples = 10\n",
    "    \n",
    "    # 設定要測試的JPEG質量級別\n",
    "    qualities = [10, 30, 50, 70]\n",
    "    \n",
    "    # 運行推理\n",
    "    run_inference(num_samples, qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
