{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Total parameters: 119,873,161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 313/313 [00:56<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.01965, MSE Loss: 0.00664, Color Loss: 0.07001, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1: 100%|██████████| 40/40 [00:03<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00516, MSE Loss: 0.00607, Color Loss: 0.06712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:01<00:00, 78.20it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 101.27it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 101.02it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 101.46it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with val loss 0.00516, MSE loss 0.00607, and color loss 0.06712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 313/313 [00:56<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.02047, MSE Loss: 0.00646, Color Loss: 0.06809, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00492, MSE Loss: 0.00579, Color Loss: 0.06476\n",
      "New best model saved with val loss 0.00492, MSE loss 0.00579, and color loss 0.06476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.02008, MSE Loss: 0.00585, Color Loss: 0.06293, LR: 9.98e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 3: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00489, MSE Loss: 0.00575, Color Loss: 0.06444\n",
      "New best model saved with val loss 0.00489, MSE loss 0.00575, and color loss 0.06444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.02069, MSE Loss: 0.00560, Color Loss: 0.06124, LR: 9.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 4: 100%|██████████| 40/40 [00:03<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00482, MSE Loss: 0.00567, Color Loss: 0.06337\n",
      "New best model saved with val loss 0.00482, MSE loss 0.00567, and color loss 0.06337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.02053, MSE Loss: 0.00517, Color Loss: 0.05765, LR: 9.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 5: 100%|██████████| 40/40 [00:03<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00467, MSE Loss: 0.00549, Color Loss: 0.06213\n",
      "New best model saved with val loss 0.00467, MSE loss 0.00549, and color loss 0.06213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.02316, MSE Loss: 0.00565, Color Loss: 0.06118, LR: 9.91e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 6: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00481, MSE Loss: 0.00566, Color Loss: 0.06274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.65it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.76it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.68it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.60it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 7: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.02493, MSE Loss: 0.00582, Color Loss: 0.06246, LR: 9.88e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 7: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00469, MSE Loss: 0.00552, Color Loss: 0.06208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.02356, MSE Loss: 0.00506, Color Loss: 0.05665, LR: 9.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 8: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00454, MSE Loss: 0.00534, Color Loss: 0.06062\n",
      "New best model saved with val loss 0.00454, MSE loss 0.00534, and color loss 0.06062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.02769, MSE Loss: 0.00589, Color Loss: 0.06302, LR: 9.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 9: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00454, MSE Loss: 0.00534, Color Loss: 0.06050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.02682, MSE Loss: 0.00533, Color Loss: 0.05866, LR: 9.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 10: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00441, MSE Loss: 0.00519, Color Loss: 0.05924\n",
      "New best model saved with val loss 0.00441, MSE loss 0.00519, and color loss 0.05924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Avg Loss: 0.02581, MSE Loss: 0.00482, Color Loss: 0.05428, LR: 9.70e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 11: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00458, MSE Loss: 0.00539, Color Loss: 0.06008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.47it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.69it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.30it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.95it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 12: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Avg Loss: 0.02634, MSE Loss: 0.00469, Color Loss: 0.05323, LR: 9.65e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 12: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00465, MSE Loss: 0.00548, Color Loss: 0.06124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Avg Loss: 0.02840, MSE Loss: 0.00492, Color Loss: 0.05505, LR: 9.59e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 13: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00468, MSE Loss: 0.00551, Color Loss: 0.06139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Avg Loss: 0.02877, MSE Loss: 0.00477, Color Loss: 0.05372, LR: 9.52e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 14: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00438, MSE Loss: 0.00515, Color Loss: 0.05839\n",
      "New best model saved with val loss 0.00438, MSE loss 0.00515, and color loss 0.05839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Avg Loss: 0.02921, MSE Loss: 0.00463, Color Loss: 0.05264, LR: 9.46e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 15: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00434, MSE Loss: 0.00511, Color Loss: 0.05854\n",
      "New best model saved with val loss 0.00434, MSE loss 0.00511, and color loss 0.05854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Avg Loss: 0.02965, MSE Loss: 0.00456, Color Loss: 0.05154, LR: 9.38e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 16: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00431, MSE Loss: 0.00507, Color Loss: 0.05820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 100.08it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.10it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.30it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.53it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with val loss 0.00431, MSE loss 0.00507, and color loss 0.05820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Avg Loss: 0.03320, MSE Loss: 0.00507, Color Loss: 0.05556, LR: 9.30e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 17: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00427, MSE Loss: 0.00502, Color Loss: 0.05744\n",
      "New best model saved with val loss 0.00427, MSE loss 0.00502, and color loss 0.05744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Avg Loss: 0.02890, MSE Loss: 0.00402, Color Loss: 0.04718, LR: 9.22e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 18: 100%|██████████| 40/40 [00:03<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00415, MSE Loss: 0.00489, Color Loss: 0.05625\n",
      "New best model saved with val loss 0.00415, MSE loss 0.00489, and color loss 0.05625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Avg Loss: 0.03073, MSE Loss: 0.00421, Color Loss: 0.04849, LR: 9.14e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 19: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00425, MSE Loss: 0.00500, Color Loss: 0.05748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Avg Loss: 0.03266, MSE Loss: 0.00434, Color Loss: 0.04994, LR: 9.05e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 20: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00417, MSE Loss: 0.00490, Color Loss: 0.05671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Avg Loss: 0.03178, MSE Loss: 0.00403, Color Loss: 0.04725, LR: 8.95e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 21: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00425, MSE Loss: 0.00501, Color Loss: 0.05775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 100.36it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 99.88it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.16it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.22it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 22: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Avg Loss: 0.03184, MSE Loss: 0.00392, Color Loss: 0.04598, LR: 8.85e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 22: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00413, MSE Loss: 0.00486, Color Loss: 0.05641\n",
      "New best model saved with val loss 0.00413, MSE loss 0.00486, and color loss 0.05641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Avg Loss: 0.03252, MSE Loss: 0.00386, Color Loss: 0.04569, LR: 8.75e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 23: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00421, MSE Loss: 0.00495, Color Loss: 0.05685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Avg Loss: 0.03258, MSE Loss: 0.00375, Color Loss: 0.04454, LR: 8.64e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 24: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00422, MSE Loss: 0.00496, Color Loss: 0.05672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Avg Loss: 0.03492, MSE Loss: 0.00397, Color Loss: 0.04639, LR: 8.54e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 25: 100%|██████████| 40/40 [00:03<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00420, MSE Loss: 0.00495, Color Loss: 0.05662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Avg Loss: 0.03265, MSE Loss: 0.00352, Color Loss: 0.04236, LR: 8.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 26: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00439, MSE Loss: 0.00516, Color Loss: 0.05849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 100.26it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.28it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.89it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.10it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 27: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Avg Loss: 0.03534, MSE Loss: 0.00379, Color Loss: 0.04460, LR: 8.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 27: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00410, MSE Loss: 0.00482, Color Loss: 0.05573\n",
      "New best model saved with val loss 0.00410, MSE loss 0.00482, and color loss 0.05573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Avg Loss: 0.03366, MSE Loss: 0.00342, Color Loss: 0.04156, LR: 8.19e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 28: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00420, MSE Loss: 0.00494, Color Loss: 0.05639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Avg Loss: 0.03638, MSE Loss: 0.00369, Color Loss: 0.04373, LR: 8.06e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 29: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00413, MSE Loss: 0.00485, Color Loss: 0.05620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Avg Loss: 0.03799, MSE Loss: 0.00379, Color Loss: 0.04457, LR: 7.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 30: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00410, MSE Loss: 0.00482, Color Loss: 0.05553\n",
      "New best model saved with val loss 0.00410, MSE loss 0.00482, and color loss 0.05553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Avg Loss: 0.03624, MSE Loss: 0.00346, Color Loss: 0.04162, LR: 7.81e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 31: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00411, MSE Loss: 0.00484, Color Loss: 0.05608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.28it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 99.80it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.00it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 99.37it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 32: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Avg Loss: 0.03683, MSE Loss: 0.00341, Color Loss: 0.04138, LR: 7.68e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 32: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00406, MSE Loss: 0.00478, Color Loss: 0.05498\n",
      "New best model saved with val loss 0.00406, MSE loss 0.00478, and color loss 0.05498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Avg Loss: 0.03641, MSE Loss: 0.00332, Color Loss: 0.03999, LR: 7.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 33: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00411, MSE Loss: 0.00483, Color Loss: 0.05608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.03458, MSE Loss: 0.00298, Color Loss: 0.03725, LR: 7.41e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 34: 100%|██████████| 40/40 [00:03<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00406, MSE Loss: 0.00478, Color Loss: 0.05533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.03894, MSE Loss: 0.00340, Color Loss: 0.04097, LR: 7.27e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 35: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00416, MSE Loss: 0.00489, Color Loss: 0.05615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.03458, MSE Loss: 0.00281, Color Loss: 0.03577, LR: 7.13e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 36: 100%|██████████| 40/40 [00:03<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00418, MSE Loss: 0.00492, Color Loss: 0.05643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.15it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.34it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.61it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.66it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 37: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.04030, MSE Loss: 0.00338, Color Loss: 0.04069, LR: 6.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 37: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00404, MSE Loss: 0.00475, Color Loss: 0.05507\n",
      "New best model saved with val loss 0.00404, MSE loss 0.00475, and color loss 0.05507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.03943, MSE Loss: 0.00317, Color Loss: 0.03909, LR: 6.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 38: 100%|██████████| 40/40 [00:03<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00405, MSE Loss: 0.00476, Color Loss: 0.05494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 0.03453, MSE Loss: 0.00257, Color Loss: 0.03369, LR: 6.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 39: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00412, MSE Loss: 0.00485, Color Loss: 0.05558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Avg Loss: 0.03702, MSE Loss: 0.00278, Color Loss: 0.03537, LR: 6.55e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 40: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00408, MSE Loss: 0.00480, Color Loss: 0.05524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Avg Loss: 0.03766, MSE Loss: 0.00278, Color Loss: 0.03529, LR: 6.39e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 41: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00405, MSE Loss: 0.00476, Color Loss: 0.05510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.69it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.64it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.67it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 101.31it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 42: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Avg Loss: 0.03882, MSE Loss: 0.00288, Color Loss: 0.03638, LR: 6.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 42: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00403, MSE Loss: 0.00474, Color Loss: 0.05498\n",
      "New best model saved with val loss 0.00403, MSE loss 0.00474, and color loss 0.05498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 313/313 [00:56<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Avg Loss: 0.03788, MSE Loss: 0.00276, Color Loss: 0.03553, LR: 6.09e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 43: 100%|██████████| 40/40 [00:03<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00408, MSE Loss: 0.00480, Color Loss: 0.05536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Avg Loss: 0.03702, MSE Loss: 0.00269, Color Loss: 0.03473, LR: 5.94e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 44: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00411, MSE Loss: 0.00484, Color Loss: 0.05567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Avg Loss: 0.03935, MSE Loss: 0.00295, Color Loss: 0.03684, LR: 5.78e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 45: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00416, MSE Loss: 0.00490, Color Loss: 0.05602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 313/313 [00:56<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Avg Loss: 0.03863, MSE Loss: 0.00289, Color Loss: 0.03617, LR: 5.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 46: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00399, MSE Loss: 0.00469, Color Loss: 0.05449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.72it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 99.91it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.10it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 99.88it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with val loss 0.00399, MSE loss 0.00469, and color loss 0.05449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Avg Loss: 0.03662, MSE Loss: 0.00265, Color Loss: 0.03437, LR: 5.47e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 47: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00402, MSE Loss: 0.00473, Color Loss: 0.05457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Avg Loss: 0.03740, MSE Loss: 0.00276, Color Loss: 0.03505, LR: 5.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 48: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00401, MSE Loss: 0.00472, Color Loss: 0.05455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Avg Loss: 0.03686, MSE Loss: 0.00268, Color Loss: 0.03457, LR: 5.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 49: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00405, MSE Loss: 0.00476, Color Loss: 0.05511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Avg Loss: 0.04022, MSE Loss: 0.00303, Color Loss: 0.03765, LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 50: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00393, MSE Loss: 0.00462, Color Loss: 0.05394\n",
      "New best model saved with val loss 0.00393, MSE loss 0.00462, and color loss 0.05394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - Avg Loss: 0.03767, MSE Loss: 0.00280, Color Loss: 0.03530, LR: 4.84e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 51: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00397, MSE Loss: 0.00467, Color Loss: 0.05436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 100.71it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.29it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.48it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.95it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 52: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 - Avg Loss: 0.03556, MSE Loss: 0.00253, Color Loss: 0.03341, LR: 4.69e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 52: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00398, MSE Loss: 0.00469, Color Loss: 0.05436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - Avg Loss: 0.03379, MSE Loss: 0.00235, Color Loss: 0.03179, LR: 4.53e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 53: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00392, MSE Loss: 0.00461, Color Loss: 0.05404\n",
      "New best model saved with val loss 0.00392, MSE loss 0.00461, and color loss 0.05404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 - Avg Loss: 0.03792, MSE Loss: 0.00282, Color Loss: 0.03552, LR: 4.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 54: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00402, MSE Loss: 0.00473, Color Loss: 0.05428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 - Avg Loss: 0.03827, MSE Loss: 0.00286, Color Loss: 0.03584, LR: 4.22e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 55: 100%|██████████| 40/40 [00:03<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00395, MSE Loss: 0.00465, Color Loss: 0.05440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56: 100%|██████████| 313/313 [00:56<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 - Avg Loss: 0.03706, MSE Loss: 0.00271, Color Loss: 0.03475, LR: 4.06e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 56: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00400, MSE Loss: 0.00471, Color Loss: 0.05451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.83it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.57it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.58it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 99.00it/s] \n",
      "Training Epoch 57: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 - Avg Loss: 0.03941, MSE Loss: 0.00298, Color Loss: 0.03687, LR: 3.91e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 57: 100%|██████████| 40/40 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00388, MSE Loss: 0.00457, Color Loss: 0.05355\n",
      "New best model saved with val loss 0.00388, MSE loss 0.00457, and color loss 0.05355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58: 100%|██████████| 313/313 [00:56<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 - Avg Loss: 0.03788, MSE Loss: 0.00281, Color Loss: 0.03549, LR: 3.76e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 58: 100%|██████████| 40/40 [00:03<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00395, MSE Loss: 0.00464, Color Loss: 0.05371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 - Avg Loss: 0.03677, MSE Loss: 0.00269, Color Loss: 0.03449, LR: 3.61e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 59: 100%|██████████| 40/40 [00:03<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00401, MSE Loss: 0.00472, Color Loss: 0.05459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 - Avg Loss: 0.03610, MSE Loss: 0.00264, Color Loss: 0.03386, LR: 3.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 60: 100%|██████████| 40/40 [00:03<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00393, MSE Loss: 0.00462, Color Loss: 0.05382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 - Avg Loss: 0.03477, MSE Loss: 0.00246, Color Loss: 0.03268, LR: 3.31e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 61: 100%|██████████| 40/40 [00:03<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00394, MSE Loss: 0.00463, Color Loss: 0.05372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 99.79it/s] \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.39it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.90it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 99.99it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 62: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 - Avg Loss: 0.03709, MSE Loss: 0.00271, Color Loss: 0.03479, LR: 3.16e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 62: 100%|██████████| 40/40 [00:03<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00391, MSE Loss: 0.00460, Color Loss: 0.05364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 63: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 - Avg Loss: 0.03638, MSE Loss: 0.00264, Color Loss: 0.03414, LR: 3.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 63: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00407, MSE Loss: 0.00479, Color Loss: 0.05521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 64: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 - Avg Loss: 0.03874, MSE Loss: 0.00291, Color Loss: 0.03627, LR: 2.87e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 64: 100%|██████████| 40/40 [00:03<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00392, MSE Loss: 0.00462, Color Loss: 0.05397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 65: 100%|██████████| 313/313 [00:56<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 - Avg Loss: 0.03847, MSE Loss: 0.00288, Color Loss: 0.03602, LR: 2.73e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 65: 100%|██████████| 40/40 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00403, MSE Loss: 0.00474, Color Loss: 0.05469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 66: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 - Avg Loss: 0.03936, MSE Loss: 0.00296, Color Loss: 0.03684, LR: 2.59e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 66: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00401, MSE Loss: 0.00472, Color Loss: 0.05419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 100.19it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 100.72it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 100.56it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 100.27it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Training Epoch 67: 100%|██████████| 313/313 [00:56<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 - Avg Loss: 0.03904, MSE Loss: 0.00292, Color Loss: 0.03656, LR: 2.45e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 67: 100%|██████████| 40/40 [00:03<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Avg Loss: 0.00394, MSE Loss: 0.00464, Color Loss: 0.05386\n",
      "Early stopping after 67 epochs!\n",
      "Training completed!\n",
      "Loaded best model from epoch 57 with val loss 0.00388\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# 設定GPU裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 資料集準備\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size]\n",
    ")\n",
    "\n",
    "# 設定資料載入器\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 定義JPEG壓縮函數\n",
    "def jpeg_compress(x, quality):\n",
    "    \"\"\"執行JPEG壓縮並保留色彩資訊\"\"\"\n",
    "    x = (x * 127.5 + 127.5).clamp(0, 255).to(torch.uint8).cpu()\n",
    "    compressed_images = []\n",
    "    for img in x:\n",
    "        pil_img = torchvision.transforms.ToPILImage()(img)\n",
    "        buffer = io.BytesIO()\n",
    "        # 確保quality在1-100的有效區間內\n",
    "        quality = max(1, min(100, int(quality)))\n",
    "        # 根據壓縮質量選擇子採樣方式，高質量時保留色彩資訊\n",
    "        subsampling = \"4:4:4\" if quality > 30 else \"4:2:0\"\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=quality, subsampling=subsampling)\n",
    "        buffer.seek(0)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        compressed_tensor = torchvision.transforms.ToTensor()(compressed_img)\n",
    "        compressed_images.append(compressed_tensor)\n",
    "    return torch.stack(compressed_images).to(device).sub(0.5).div(0.5)\n",
    "\n",
    "# 時間嵌入模組\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.proj(emb)\n",
    "\n",
    "# DCT變換層\n",
    "class DCTLayer(nn.Module):\n",
    "    \"\"\"執行DCT變換操作，處理頻率域信息\"\"\"\n",
    "    def __init__(self, block_size=8):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 執行DCT變換並確保輸出尺寸與輸入一致\n",
    "        x_dct = self._apply_dct(x)\n",
    "        b, c, h, w = x.shape\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        return x_dct\n",
    "    \n",
    "    def _apply_dct(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # 填充至block_size的整數倍\n",
    "        h_pad = (self.block_size - h % self.block_size) % self.block_size\n",
    "        w_pad = (self.block_size - w % self.block_size) % self.block_size\n",
    "        \n",
    "        x_padded = F.pad(x, (0, w_pad, 0, h_pad))\n",
    "        _, _, h_padded, w_padded = x_padded.shape\n",
    "        \n",
    "        # 計算區塊數量\n",
    "        h_blocks = h_padded // self.block_size\n",
    "        w_blocks = w_padded // self.block_size\n",
    "        \n",
    "        # 分割圖像\n",
    "        x_blocks = x_padded.unfold(2, self.block_size, self.block_size).unfold(3, self.block_size, self.block_size)\n",
    "        \n",
    "        # 變形為適合DCT的形狀\n",
    "        b_unf, c_unf, h_unf, w_unf, bs_h, bs_w = x_blocks.shape\n",
    "        x_blocks_flat = x_blocks.reshape(-1, self.block_size, self.block_size)\n",
    "        \n",
    "        # 獲取DCT矩陣\n",
    "        dct_matrix = self._get_dct_matrix(self.block_size).to(x.device)\n",
    "        \n",
    "        # 應用DCT變換: D * X * D^T\n",
    "        x_dct_flat = torch.matmul(dct_matrix, x_blocks_flat)\n",
    "        x_dct_flat = torch.matmul(x_dct_flat, dct_matrix.transpose(0, 1))\n",
    "        \n",
    "        # 還原形狀\n",
    "        x_dct_blocks = x_dct_flat.reshape(b_unf, c_unf, h_unf, w_unf, bs_h, bs_w)\n",
    "        \n",
    "        # 重新排列並還原為原始形狀\n",
    "        x_dct_perm = x_dct_blocks.permute(0, 1, 2, 4, 3, 5)\n",
    "        x_dct = x_dct_perm.reshape(b, c, h_padded, w_padded)\n",
    "        \n",
    "        # 移除填充部分\n",
    "        if h_pad > 0 or w_pad > 0:\n",
    "            x_dct = x_dct[:, :, :h, :w]\n",
    "        \n",
    "        return x_dct\n",
    "    \n",
    "    def _get_dct_matrix(self, size):\n",
    "        \"\"\"生成標準DCT變換矩陣\"\"\"\n",
    "        dct_matrix = torch.zeros(size, size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if i == 0:\n",
    "                    dct_matrix[i, j] = 1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32))\n",
    "                else:\n",
    "                    dct_matrix[i, j] = torch.sqrt(torch.tensor(2.0 / size)) * torch.cos(torch.tensor(torch.pi * (2 * j + 1) * i / (2 * size)))\n",
    "        return dct_matrix\n",
    "\n",
    "# 高頻增強模組 - 適用於JPEG壓縮恢復\n",
    "class HFCM(nn.Module):\n",
    "    \"\"\"高頻增強模組，參考FDG-Diff論文方法\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dct = DCTLayer(block_size=8)\n",
    "        self.high_freq_attn = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(channels, channels, 1)\n",
    "        \n",
    "    def forward(self, x, compression_level):\n",
    "        # 獲取DCT頻率表示\n",
    "        x_dct = self.dct(x)\n",
    "        \n",
    "        # 確保x_dct與x具有相同的空間尺寸\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 高頻注意力機制\n",
    "        attn_mask = self.high_freq_attn(x)\n",
    "        \n",
    "        # 根據compression_level調整權重\n",
    "        if isinstance(compression_level, torch.Tensor) and compression_level.dim() > 0:\n",
    "            compression_level = compression_level.view(-1, 1, 1, 1)\n",
    "        \n",
    "        # 壓縮程度越高(值越大)，保留的高頻越少\n",
    "        freq_scale = 1.0 - compression_level\n",
    "        \n",
    "        # 應用高頻增強\n",
    "        enhanced = x + attn_mask * x_dct * freq_scale\n",
    "        return self.conv_out(enhanced)\n",
    "\n",
    "# 頻率感知塊\n",
    "class FrequencyAwareBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dct_layer = DCTLayer(block_size=8)\n",
    "        self.freq_conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.freq_attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // 4, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // 4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, compression_level):\n",
    "        # 獲取頻率表示\n",
    "        x_dct = self.dct_layer(x)\n",
    "        \n",
    "        # 確保x_dct與x具有相同的空間尺寸\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "        x_freq = self.freq_conv(x_dct)\n",
    "        \n",
    "        # 確保x_freq與x具有相同的空間尺寸\n",
    "        if x_freq.shape[2:] != x.shape[2:]:\n",
    "            x_freq = F.interpolate(x_freq, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 生成頻率注意力圖\n",
    "        attn = self.freq_attn(x_freq)\n",
    "        \n",
    "        # 根據compression_level調整注意力權重\n",
    "        if isinstance(compression_level, torch.Tensor) and compression_level.dim() > 0:\n",
    "            compression_level = compression_level.view(-1, 1, 1, 1)\n",
    "        \n",
    "        # 壓縮程度越高，注意力權重越低\n",
    "        attn = attn * (1.0 - compression_level) + 0.5\n",
    "        \n",
    "        # 確保attn與x_freq尺寸一致\n",
    "        if attn.shape[2:] != x_freq.shape[2:]:\n",
    "            attn = F.interpolate(attn, size=x_freq.shape[2:], mode='nearest')\n",
    "        \n",
    "        # 應用頻率注意力\n",
    "        return x + x_freq * attn\n",
    "\n",
    "# 殘差注意力塊\n",
    "class ResAttnBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim, dropout=0.1, use_freq_guide=False):\n",
    "        super().__init__()\n",
    "        # 確保組數適合通道數\n",
    "        num_groups = min(8, in_c)\n",
    "        while in_c % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "            \n",
    "        self.norm1 = nn.GroupNorm(num_groups, in_c)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.time_proj = nn.Linear(time_dim, out_c)\n",
    "        \n",
    "        # 調整 out_c 的組數\n",
    "        num_groups_out = min(8, out_c)\n",
    "        while out_c % num_groups_out != 0 and num_groups_out > 1:\n",
    "            num_groups_out -= 1\n",
    "            \n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.attn = nn.MultiheadAttention(out_c, 4, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "        \n",
    "        # 頻率增強(可選使用)\n",
    "        self.use_freq_guide = use_freq_guide\n",
    "        if use_freq_guide:\n",
    "            self.freq_guide = FrequencyAwareBlock(out_c)\n",
    "            self.hfcm = HFCM(out_c)\n",
    "        \n",
    "    def forward(self, x, t_emb, compression_level=None):\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        # 加入時間編碼\n",
    "        t = self.time_proj(t_emb)[..., None, None]\n",
    "        h = h + t\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = self.conv2(F.silu(h))\n",
    "        \n",
    "        # 應用自注意力機制\n",
    "        b, c, hh, ww = h.shape\n",
    "        h_attn = h.view(b, c, -1).permute(0, 2, 1)\n",
    "        h_attn, _ = self.attn(h_attn, h_attn, h_attn)\n",
    "        h_attn = h_attn.permute(0, 2, 1).view(b, c, hh, ww)\n",
    "        \n",
    "        # 應用頻率增強(如果啟用)\n",
    "        if self.use_freq_guide and compression_level is not None:\n",
    "            h_attn = self.freq_guide(h_attn, compression_level)\n",
    "            h_attn = self.hfcm(h_attn, compression_level)\n",
    "        \n",
    "        return self.shortcut(x) + self.dropout(h_attn)\n",
    "\n",
    "# JPEG擴散模型\n",
    "class JPEGDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        time_dim = 256\n",
    "        self.time_embed = TimeEmbedding(time_dim)\n",
    "        \n",
    "        # 下採樣路徑 - 增強頻率感知\n",
    "        self.down1 = ResAttnBlock(3, 64, time_dim)\n",
    "        self.down2 = ResAttnBlock(64, 128, time_dim, use_freq_guide=True)\n",
    "        self.down3 = ResAttnBlock(128, 256, time_dim, use_freq_guide=True)\n",
    "        self.down4 = ResAttnBlock(256, 512, time_dim)\n",
    "        self.down5 = ResAttnBlock(512, 512, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 瓶頸層 - 使用頻率增強\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResAttnBlock(512, 1024, time_dim, use_freq_guide=True),\n",
    "            ResAttnBlock(1024, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 512, time_dim, use_freq_guide=True)\n",
    "        )\n",
    "        \n",
    "        # 上採樣路徑 - 增強頻率感知\n",
    "        self.up1 = ResAttnBlock(1024, 512, time_dim)\n",
    "        self.up2 = ResAttnBlock(512 + 512, 256, time_dim, use_freq_guide=True)\n",
    "        self.up3 = ResAttnBlock(256 + 256, 128, time_dim, use_freq_guide=True)\n",
    "        self.up4 = ResAttnBlock(128 + 128, 64, time_dim)\n",
    "        self.up5 = ResAttnBlock(64 + 64, 64, time_dim)\n",
    "        \n",
    "        # 輸出層 - 使用1x1卷積產生空間色彩分布\n",
    "        self.out_conv = nn.Conv2d(64, 3, 1)\n",
    "        \n",
    "    def forward(self, x, t, compression_level=None):\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # 如果未提供壓縮程度，使用時間步長代替\n",
    "        if compression_level is None:\n",
    "            compression_level = t.clone().detach()\n",
    "        \n",
    "        # 下採樣\n",
    "        d1 = self.down1(x, t_emb)  # 32x32\n",
    "        d2 = self.down2(self.pool(d1), t_emb, compression_level)  # 16x16\n",
    "        d3 = self.down3(self.pool(d2), t_emb, compression_level)  # 8x8\n",
    "        d4 = self.down4(self.pool(d3), t_emb)  # 4x4\n",
    "        d5 = self.down5(self.pool(d4), t_emb)  # 2x2\n",
    "        \n",
    "        # 瓶頸層\n",
    "        b = self.bottleneck[0](self.pool(d5), t_emb, compression_level)\n",
    "        b = self.bottleneck[1](b, t_emb)\n",
    "        b = self.bottleneck[2](b, t_emb, compression_level)\n",
    "        \n",
    "        # 上採樣 - 使用連接操作將特徵合併回來\n",
    "        u1 = self.up1(torch.cat([F.interpolate(b, scale_factor=2, mode='bilinear', align_corners=False), d5], dim=1), t_emb)\n",
    "        u2 = self.up2(torch.cat([F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False), d4], dim=1), t_emb, compression_level)\n",
    "        u3 = self.up3(torch.cat([F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False), d3], dim=1), t_emb, compression_level)\n",
    "        u4 = self.up4(torch.cat([F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False), d2], dim=1), t_emb)\n",
    "        u5 = self.up5(torch.cat([F.interpolate(u4, scale_factor=2, mode='bilinear', align_corners=False), d1], dim=1), t_emb)\n",
    "        \n",
    "        return self.out_conv(u5)\n",
    "\n",
    "# 定義前向過程(JPEG壓縮加噪)\n",
    "def forward_process(x0, t, quality_factors=None):\n",
    "    \"\"\"使用類似DriftRec的前向SDE實現JPEG壓縮\"\"\"\n",
    "    b = x0.size(0)\n",
    "    \n",
    "    # 如果未提供quality_factors，根據時間步長計算\n",
    "    if quality_factors is None:\n",
    "        # 隨時間步長變化調整壓縮質量(1-100)，t越大，質量越低\n",
    "        quality_factors = torch.clamp(100 * (1 - t.float() / num_timesteps), 1, 100).cpu().numpy()\n",
    "    \n",
    "    # JPEG壓縮\n",
    "    xt = torch.stack([jpeg_compress(x0[i:i+1], int(q)) for i, q in enumerate(quality_factors)]).squeeze()\n",
    "    \n",
    "    # 添加少量高斯噪聲以增強穩定性(DriftRec建議)\n",
    "    noise_scale = 0.01 * t.float() / num_timesteps  # 隨時間步長強度增加\n",
    "    xt = xt + noise_scale.view(-1, 1, 1, 1) * torch.randn_like(xt)\n",
    "    \n",
    "    return xt\n",
    "\n",
    "# 設置擴散參數\n",
    "num_timesteps = 100\n",
    "# 根據DriftRec論文，使用線性增加的噪聲\n",
    "betas = torch.linspace(1e-4, 0.02, num_timesteps).to(device)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# 初始化模型\n",
    "model = JPEGDiffusionModel().to(device)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 優化器設置 - 使用AdamW以獲得更好收斂性能\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5, betas=(0.9, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)\n",
    "\n",
    "# 損失函數 - 標準MSE損失(符合DDPM論文) + 自定義損失\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "huber_loss_fn = nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "\n",
    "# 色彩保持損失函數\n",
    "def color_preservation_loss(pred, target):\n",
    "    \"\"\"色彩保持損失，加強RGB通道保真度\"\"\"\n",
    "    # 將張量放縮回[0,1]的範圍\n",
    "    pred = (pred * 0.5 + 0.5).clamp(0, 1)\n",
    "    target = (target * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    # 計算RGB差異，給予色彩通道不同權重\n",
    "    r_loss = F.l1_loss(pred[:, 0], target[:, 0])\n",
    "    g_loss = F.l1_loss(pred[:, 1], target[:, 1])\n",
    "    b_loss = F.l1_loss(pred[:, 2], target[:, 2])\n",
    "    \n",
    "    # 綠色通道對感知影響最大，給予更高權重\n",
    "    color_loss = 0.25 * r_loss + 0.5 * g_loss + 0.25 * b_loss\n",
    "    \n",
    "    # 額外添加SSIM感知損失\n",
    "    ssim_loss = 1 - ssim(pred, target, data_range=1.0, size_average=True)\n",
    "    \n",
    "    # 總損失\n",
    "    return color_loss + 0.5 * ssim_loss\n",
    "\n",
    "# 高斯混合採樣器(基於GM-DDPM)\n",
    "class GaussianMixtureSampler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def sample(self, x_t, steps=100, use_phase_consistency=True, use_svd_guide=True, guidance_scale=1.0):\n",
    "        \"\"\"使用高斯混合採樣進行推理，結合頻率一致性和SVD引導\"\"\"\n",
    "        self.model.eval()\n",
    "        # 保存初始壓縮影像作為頻率一致性基準\n",
    "        original_compressed = x_t.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 從給定的噪聲影像開始\n",
    "            for i in tqdm(range(steps-1, -1, -1), desc=\"Sampling\"):\n",
    "                t = torch.full((x_t.size(0),), i, device=device).float() / num_timesteps\n",
    "                compression_level = t.clone()  # 壓縮程度與時間步長關聯\n",
    "                \n",
    "                # 獲取噪聲預測\n",
    "                pred_noise = model(x_t, t, compression_level)\n",
    "                \n",
    "                # 可選使用SVD引導\n",
    "                if use_svd_guide and i > steps // 2:\n",
    "                    # 在採樣前半部分使用SVD引導(結構保存)\n",
    "                    k_ratio = i / steps  # 隨時間步長調整重要度\n",
    "                    structure_prior = svd_structure_preservation(x_t, k_ratio)\n",
    "                    # 混合SVD結構先驗與預測\n",
    "                    guide_strength = k_ratio * 0.3  # SVD權重隨時間步長調整\n",
    "                    pred_noise = (1 - guide_strength) * pred_noise + guide_strength * (original_compressed - structure_prior)\n",
    "                \n",
    "                if i > 0:\n",
    "                    # 預測的x0\n",
    "                    x0_pred = x_t + pred_noise\n",
    "                    \n",
    "                    # 計算高斯混合模型的兩個均值\n",
    "                    # 第一個均值 - 偏向原生的預測\n",
    "                    mu1 = x0_pred * 0.9 + x_t * 0.1\n",
    "                    # 第二個均值 - 偏向更激進的預測\n",
    "                    mu2 = x0_pred * 1.1 - x_t * 0.1\n",
    "                    \n",
    "                    # 根據當前時間步長動態使用不同均值\n",
    "                    # 時間步長較大時偏向保守，較小時偏向激進\n",
    "                    p_conservative = max(0.2, min(0.8, i / steps))\n",
    "                    use_first = torch.rand(1).item() < p_conservative\n",
    "                    next_mean = mu1 if use_first else mu2\n",
    "                    \n",
    "                    # 添加適量高斯噪聲\n",
    "                    noise_scale = 0.1 * i / steps * guidance_scale\n",
    "                    x_next = next_mean + noise_scale * torch.randn_like(x_t)\n",
    "                    \n",
    "                    # 頻率一致性保持(每5步使用一次)\n",
    "                    if use_phase_consistency and i % 5 == 0:\n",
    "                        alpha = 0.6 + 0.3 * (1 - i / steps)  # 隨時間步長加強初始影像權重\n",
    "                        x_next = phase_consistency(x_next, original_compressed, alpha)\n",
    "                    \n",
    "                    x_t = x_next\n",
    "                else:\n",
    "                    # 最後一步直接使用預測的去噪結果\n",
    "                    x_t = x_t + pred_noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "# SVD結構保持函數\n",
    "def svd_structure_preservation(x, k_ratio=0.5):\n",
    "    \"\"\"使用SVD保持主要結構特徵\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    x_flat = x.view(b, c, -1)\n",
    "    \n",
    "    # 對每個通道分別進行SVD分解\n",
    "    structure_tensors = []\n",
    "    for i in range(b):\n",
    "        channels_structure = []\n",
    "        for j in range(c):\n",
    "            # SVD分解\n",
    "            U, S, Vh = torch.linalg.svd(x_flat[i, j].view(h, w), full_matrices=False)\n",
    "            \n",
    "            # 確定保留的奇異值數量\n",
    "            k = max(1, int(min(h, w) * k_ratio))\n",
    "            \n",
    "            # 重建低秩表示\n",
    "            S_k = torch.zeros_like(S)\n",
    "            S_k[:k] = S[:k]\n",
    "            structure = torch.matmul(U, torch.matmul(torch.diag(S_k), Vh))\n",
    "            channels_structure.append(structure.unsqueeze(0))\n",
    "        \n",
    "        # 合併通道結構\n",
    "        structure_tensors.append(torch.cat(channels_structure, dim=0).unsqueeze(0))\n",
    "    \n",
    "    return torch.cat(structure_tensors, dim=0)\n",
    "\n",
    "# 相位一致性函數\n",
    "def phase_consistency(x, ref, alpha=0.7):\n",
    "    \"\"\"使用傅里葉變換的相位一致性，保持頻域特性\"\"\"\n",
    "    # FFT變換\n",
    "    x_fft = torch.fft.fft2(x)\n",
    "    ref_fft = torch.fft.fft2(ref)\n",
    "    \n",
    "    # 獲取幅度和相位\n",
    "    x_mag = torch.abs(x_fft)\n",
    "    ref_phase = torch.angle(ref_fft)\n",
    "    \n",
    "    # 融合x的幅度和ref的相位\n",
    "    real = x_mag * torch.cos(ref_phase)\n",
    "    imag = x_mag * torch.sin(ref_phase)\n",
    "    adjusted_fft = torch.complex(real, imag)\n",
    "    \n",
    "    # 逆變換\n",
    "    adjusted_img = torch.fft.ifft2(adjusted_fft).real\n",
    "    \n",
    "    # 混合原始影像和相位調整影像\n",
    "    return alpha * x + (1 - alpha) * adjusted_img\n",
    "\n",
    "# 訓練一個epoch\n",
    "def train_epoch(model, loader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    mse_loss_total = 0\n",
    "    color_loss_total = 0\n",
    "    \n",
    "    for x0, _ in tqdm(loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        x0 = x0.to(device)\n",
    "        b = x0.size(0)\n",
    "        \n",
    "        # 使用更符合現實的質量選擇策略\n",
    "        # 根據模型訓練進度增加高質量JPEG比例\n",
    "        if random.random() < 0.3 + min(0.4, epoch * 0.01):  # 隨訓練增加高質量JPEG比例\n",
    "            # 高質量壓縮\n",
    "            quality_range = (70, 100)\n",
    "        elif random.random() < 0.5:\n",
    "            # 中等質量壓縮\n",
    "            quality_range = (40, 70)\n",
    "        else:\n",
    "            # 低質量壓縮\n",
    "            quality_range = (5, 40)\n",
    "            \n",
    "        # 隨機選擇時間步長\n",
    "        t = torch.randint(1, num_timesteps, (b,), device=device).long()\n",
    "        \n",
    "        # 根據時間步長計算質量範圍，得到具體的壓縮質量\n",
    "        min_q, max_q = quality_range\n",
    "        quality = torch.clamp(min_q + (max_q - min_q) * (1 - t.float() / num_timesteps), 1, 100).cpu().numpy()\n",
    "        \n",
    "        # 使用自定義前向過程\n",
    "        xt = forward_process(x0, t, quality)\n",
    "        \n",
    "        # 計算噪聲 (x0 - xt)\n",
    "        noise = x0 - xt\n",
    "        \n",
    "        # 模型預測噪聲\n",
    "        compression_level = t.float() / num_timesteps  # 壓縮程度\n",
    "        pred_noise = model(xt, t.float()/num_timesteps, compression_level)\n",
    "        \n",
    "        # 計算MSE損失(DDPM標準損失)\n",
    "        mse_loss = mse_loss_fn(pred_noise, noise)\n",
    "        \n",
    "        # 計算Huber損失(更穩健)\n",
    "        huber_loss = huber_loss_fn(pred_noise, noise)\n",
    "        \n",
    "        # 色彩保持損失\n",
    "        col_loss = color_preservation_loss(xt + pred_noise, x0)\n",
    "        \n",
    "        # 結合損失，隨訓練進度增加色彩損失權重\n",
    "        color_weight = min(1.0, 0.2 + epoch * 0.02)\n",
    "        loss = 0.7 * mse_loss + 0.3 * huber_loss + color_weight * col_loss\n",
    "        \n",
    "        # 反向傳播更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        mse_loss_total += mse_loss.item()\n",
    "        color_loss_total += col_loss.item()\n",
    "    \n",
    "    # 更新學習率\n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mse_loss = mse_loss_total / len(loader)\n",
    "    avg_color_loss = color_loss_total / len(loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.5f}, MSE Loss: {avg_mse_loss:.5f}, Color Loss: {avg_color_loss:.5f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    return avg_loss, avg_mse_loss, avg_color_loss\n",
    "\n",
    "# 驗證函數\n",
    "def validate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    mse_loss_total = 0\n",
    "    color_loss_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x0, _ in tqdm(loader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            x0 = x0.to(device)\n",
    "            b = x0.size(0)\n",
    "            \n",
    "            # 選擇隨機質量做驗證\n",
    "            quality = torch.randint(10, 90, (b,)).cpu().numpy()\n",
    "            t = torch.full((b,), num_timesteps//2, device=device).long()\n",
    "            \n",
    "            # 使用前向過程\n",
    "            xt = forward_process(x0, t, quality)\n",
    "            \n",
    "            # 計算噪聲\n",
    "            noise = x0 - xt\n",
    "            \n",
    "            # 模型預測噪聲\n",
    "            compression_level = t.float() / num_timesteps\n",
    "            pred_noise = model(xt, t.float()/num_timesteps, compression_level)\n",
    "            \n",
    "            # 計算損失\n",
    "            mse_loss = mse_loss_fn(pred_noise, noise)\n",
    "            huber_loss = huber_loss_fn(pred_noise, noise)\n",
    "            col_loss = color_preservation_loss(xt + pred_noise, x0)\n",
    "            \n",
    "            total_loss += (0.7 * mse_loss + 0.3 * huber_loss).item()\n",
    "            mse_loss_total += mse_loss.item()\n",
    "            color_loss_total += col_loss.item()\n",
    "            \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mse_loss = mse_loss_total / len(loader)\n",
    "    avg_color_loss = color_loss_total / len(loader)\n",
    "    print(f\"Validation - Avg Loss: {avg_loss:.5f}, MSE Loss: {avg_mse_loss:.5f}, Color Loss: {avg_color_loss:.5f}\")\n",
    "    \n",
    "    # 定期可視化結果\n",
    "    if epoch % 5 == 0:\n",
    "        visualize_restoration(model, epoch)\n",
    "    \n",
    "    return avg_loss, avg_mse_loss, avg_color_loss\n",
    "\n",
    "# 可視化還原效果\n",
    "def visualize_restoration(model, epoch):\n",
    "    model.eval()\n",
    "    sampler = GaussianMixtureSampler(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0, _ = next(iter(test_dataloader))\n",
    "        x0 = x0.to(device)\n",
    "        \n",
    "        # 測試不同的質量級別\n",
    "        qualities = [10, 30, 50, 70]\n",
    "        plt.figure(figsize=(len(qualities)*3+3, 6))\n",
    "        \n",
    "        # 顯示原始影像\n",
    "        plt.subplot(2, len(qualities)+1, 1)\n",
    "        plt.imshow(x0[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 對每個質量級別顯示JPEG和還原結果\n",
    "        for i, q in enumerate(qualities):\n",
    "            # JPEG壓縮\n",
    "            xt = jpeg_compress(x0, q)\n",
    "            \n",
    "            # 設定初始時間步長對應質量\n",
    "            init_t = int((100 - q) / 100 * num_timesteps)\n",
    "            \n",
    "            # 使用GMM採樣器進行還原\n",
    "            restored = sampler.sample(xt, steps=init_t+1)\n",
    "            \n",
    "            # 顯示JPEG壓縮結果\n",
    "            plt.subplot(2, len(qualities)+1, i+2)\n",
    "            plt.imshow(xt[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "            plt.title(f\"JPEG Q{q}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # 顯示還原結果\n",
    "            plt.subplot(2, len(qualities)+1, len(qualities)+i+2)\n",
    "            plt.imshow(restored[0].cpu().permute(1,2,0)*0.5+0.5)\n",
    "            plt.title(f\"Restored Q{q}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'viz_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "\n",
    "# 訓練模型主函數\n",
    "def train_model(epochs=100, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    mse_losses = []\n",
    "    color_losses = []\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 訓練一個周期\n",
    "        train_loss, train_mse_loss, train_color_loss = train_epoch(model, train_dataloader, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 驗證一個周期\n",
    "        val_loss, val_mse_loss, val_color_loss = validate(model, valid_dataloader, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "        mse_losses.append(val_mse_loss)\n",
    "        color_losses.append(val_color_loss)\n",
    "        \n",
    "        # 保存模型如果驗證損失有改善\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'mse_loss': val_mse_loss,\n",
    "                'color_loss': val_color_loss\n",
    "            }, f\"best_jpeg_diffusion.pth\")\n",
    "            print(f\"New best model saved with val loss {val_loss:.5f}, MSE loss {val_mse_loss:.5f}, and color loss {val_color_loss:.5f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs!\")\n",
    "                break\n",
    "        \n",
    "        # 繪製訓練曲線\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(mse_losses, label='MSE Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(color_losses, label='Color Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Color Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # 載入最佳模型\n",
    "    checkpoint = torch.load(\"best_jpeg_diffusion.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with val loss {checkpoint['val_loss']:.5f}\")\n",
    "\n",
    "# 執行訓練\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
      "載入模型權重從Epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 93.78it/s], ?it/s]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.89it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.77it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 95.01it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.50it/s]:52:28,  2.84s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.38it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.77it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.06it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 93.68it/s]:29:27,  3.06s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 93.86it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 93.61it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 93.70it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 93.03it/s]:10:32,  2.94s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 93.42it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 93.77it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 93.44it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 93.34it/s]:02:16,  2.89s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.31it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.38it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.09it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.10it/s]:56:37,  2.86s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 92.89it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.44it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.49it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 93.91it/s]:57:25,  2.87s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.37it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.46it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.62it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.16it/s]:53:28,  2.84s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.53it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.43it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.29it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.10it/s]:50:59,  2.83s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.39it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.57it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.29it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.35it/s]:49:04,  2.82s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.85it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.93it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.71it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.51it/s]7:51:17,  2.83s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.68it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.20it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.69it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.52it/s]7:49:01,  2.82s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.09it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.67it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.36it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.48it/s]7:47:37,  2.81s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.85it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.62it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 93.87it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.60it/s]7:46:18,  2.80s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.50it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.67it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.46it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.02it/s]7:45:23,  2.80s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.71it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.50it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.56it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.48it/s]7:45:01,  2.79s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.97it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.17it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.54it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.95it/s]7:49:41,  2.82s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.35it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.67it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.87it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.21it/s]7:48:19,  2.81s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.80it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.64it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.57it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.71it/s]7:46:48,  2.81s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.72it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.00it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.56it/s]\n",
      "Sampling: 100%|██████████| 91/91 [00:00<00:00, 94.61it/s]7:45:42,  2.80s/it]\n",
      "Sampling: 100%|██████████| 71/71 [00:00<00:00, 94.61it/s]\n",
      "Sampling: 100%|██████████| 51/51 [00:00<00:00, 94.51it/s]\n",
      "Sampling: 100%|██████████| 31/31 [00:00<00:00, 94.87it/s]\n",
      "Processing test images:   0%|          | 20/10000 [00:56<7:50:30,  2.83s/it]\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 36074 (\\N{CJK UNIFIED IDEOGRAPH-8CEA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 25552 (\\N{CJK UNIFIED IDEOGRAPH-63D0}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 21319 (\\N{CJK UNIFIED IDEOGRAPH-5347}) missing from current font.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 結果摘要 ====\n",
      "質量         PSNR (壓縮)       PSNR (還原)       SSIM (壓縮)       SSIM (還原)      \n",
      "----------------------------------------------------------------------\n",
      "10         22.74           19.37           0.7784          0.6913         \n",
      "30         26.21           23.12           0.8814          0.8386         \n",
      "50         28.82           25.37           0.9283          0.8919         \n",
      "70         30.84           27.47           0.9518          0.9191         \n",
      "\n",
      "LPIPS 結果 (越低越好):\n",
      "質量         LPIPS (壓縮)      LPIPS (還原)     \n",
      "----------------------------------------\n",
      "10         0.0448          0.0927         \n",
      "30         0.0160          0.0488         \n",
      "50         0.0044          0.0269         \n",
      "70         0.0020          0.0125         \n",
      "\n",
      "所有結果已保存至 ./inference_results 目錄\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 21892 (\\N{CJK UNIFIED IDEOGRAPH-5584}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 20540 (\\N{CJK UNIFIED IDEOGRAPH-503C}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 36234 (\\N{CJK UNIFIED IDEOGRAPH-8D8A}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:640: UserWarning: Glyph 22909 (\\N{CJK UNIFIED IDEOGRAPH-597D}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 36074 (\\N{CJK UNIFIED IDEOGRAPH-8CEA}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 25552 (\\N{CJK UNIFIED IDEOGRAPH-63D0}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 21319 (\\N{CJK UNIFIED IDEOGRAPH-5347}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 25913 (\\N{CJK UNIFIED IDEOGRAPH-6539}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 21892 (\\N{CJK UNIFIED IDEOGRAPH-5584}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 20540 (\\N{CJK UNIFIED IDEOGRAPH-503C}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 36234 (\\N{CJK UNIFIED IDEOGRAPH-8D8A}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n",
      "/tmp/ipykernel_3885563/852833859.py:641: UserWarning: Glyph 22909 (\\N{CJK UNIFIED IDEOGRAPH-597D}) missing from current font.\n",
      "  plt.savefig(f'{output_path}/performance_gains.png')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pytorch_msssim import ssim\n",
    "import lpips\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 載入您切好的CIFAR10測試數據\n",
    "def load_test_data(test_data_path, batch_size=1):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # 若您有自定義的測試數據集類，請替換下面的代碼\n",
    "    # 例如：test_dataset = YourCustomDataset(test_data_path, transform=transform)\n",
    "    \n",
    "    # 默認使用CIFAR10\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=test_data_path, train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return test_dataloader\n",
    "\n",
    "# JPEG壓縮函數\n",
    "def jpeg_compress(x, quality):\n",
    "    \"\"\"執行JPEG壓縮並保留色彩資訊\"\"\"\n",
    "    x = (x * 127.5 + 127.5).clamp(0, 255).to(torch.uint8).cpu()\n",
    "    compressed_images = []\n",
    "    for img in x:\n",
    "        pil_img = torchvision.transforms.ToPILImage()(img)\n",
    "        buffer = io.BytesIO()\n",
    "        # 確保quality在1-100的有效區間內\n",
    "        quality = max(1, min(100, int(quality)))\n",
    "        # 根據壓縮質量選擇子採樣方式，高質量時保留色彩資訊\n",
    "        subsampling = \"4:4:4\" if quality > 30 else \"4:2:0\"\n",
    "        pil_img.save(buffer, format=\"JPEG\", quality=quality, subsampling=subsampling)\n",
    "        buffer.seek(0)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        compressed_tensor = torchvision.transforms.ToTensor()(compressed_img)\n",
    "        compressed_images.append(compressed_tensor)\n",
    "    return torch.stack(compressed_images).to(device).sub(0.5).div(0.5)\n",
    "\n",
    "# 時間嵌入模組\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return self.proj(emb)\n",
    "\n",
    "# DCT變換層\n",
    "class DCTLayer(nn.Module):\n",
    "    \"\"\"執行DCT變換操作，處理頻率域信息\"\"\"\n",
    "    def __init__(self, block_size=8):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 執行DCT變換並確保輸出尺寸與輸入一致\n",
    "        x_dct = self._apply_dct(x)\n",
    "        b, c, h, w = x.shape\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        return x_dct\n",
    "    \n",
    "    def _apply_dct(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # 填充至block_size的整數倍\n",
    "        h_pad = (self.block_size - h % self.block_size) % self.block_size\n",
    "        w_pad = (self.block_size - w % self.block_size) % self.block_size\n",
    "        \n",
    "        x_padded = F.pad(x, (0, w_pad, 0, h_pad))\n",
    "        _, _, h_padded, w_padded = x_padded.shape\n",
    "        \n",
    "        # 計算區塊數量\n",
    "        h_blocks = h_padded // self.block_size\n",
    "        w_blocks = w_padded // self.block_size\n",
    "        \n",
    "        # 分割圖像\n",
    "        x_blocks = x_padded.unfold(2, self.block_size, self.block_size).unfold(3, self.block_size, self.block_size)\n",
    "        \n",
    "        # 變形為適合DCT的形狀\n",
    "        b_unf, c_unf, h_unf, w_unf, bs_h, bs_w = x_blocks.shape\n",
    "        x_blocks_flat = x_blocks.reshape(-1, self.block_size, self.block_size)\n",
    "        \n",
    "        # 獲取DCT矩陣\n",
    "        dct_matrix = self._get_dct_matrix(self.block_size).to(x.device)\n",
    "        \n",
    "        # 應用DCT變換: D * X * D^T\n",
    "        x_dct_flat = torch.matmul(dct_matrix, x_blocks_flat)\n",
    "        x_dct_flat = torch.matmul(x_dct_flat, dct_matrix.transpose(0, 1))\n",
    "        \n",
    "        # 還原形狀\n",
    "        x_dct_blocks = x_dct_flat.reshape(b_unf, c_unf, h_unf, w_unf, bs_h, bs_w)\n",
    "        \n",
    "        # 重新排列並還原為原始形狀\n",
    "        x_dct_perm = x_dct_blocks.permute(0, 1, 2, 4, 3, 5)\n",
    "        x_dct = x_dct_perm.reshape(b, c, h_padded, w_padded)\n",
    "        \n",
    "        # 移除填充部分\n",
    "        if h_pad > 0 or w_pad > 0:\n",
    "            x_dct = x_dct[:, :, :h, :w]\n",
    "        \n",
    "        return x_dct\n",
    "    \n",
    "    def _get_dct_matrix(self, size):\n",
    "        \"\"\"生成標準DCT變換矩陣\"\"\"\n",
    "        dct_matrix = torch.zeros(size, size)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if i == 0:\n",
    "                    dct_matrix[i, j] = 1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32))\n",
    "                else:\n",
    "                    dct_matrix[i, j] = torch.sqrt(torch.tensor(2.0 / size)) * torch.cos(torch.tensor(torch.pi * (2 * j + 1) * i / (2 * size)))\n",
    "        return dct_matrix\n",
    "\n",
    "# 高頻增強模組\n",
    "class HFCM(nn.Module):\n",
    "    \"\"\"高頻增強模組，參考FDG-Diff論文方法\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dct = DCTLayer(block_size=8)\n",
    "        self.high_freq_attn = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(channels, channels, 1)\n",
    "        \n",
    "    def forward(self, x, compression_level):\n",
    "        # 獲取DCT頻率表示\n",
    "        x_dct = self.dct(x)\n",
    "        \n",
    "        # 確保x_dct與x具有相同的空間尺寸\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 高頻注意力機制\n",
    "        attn_mask = self.high_freq_attn(x)\n",
    "        \n",
    "        # 根據compression_level調整權重\n",
    "        if isinstance(compression_level, torch.Tensor) and compression_level.dim() > 0:\n",
    "            compression_level = compression_level.view(-1, 1, 1, 1)\n",
    "        \n",
    "        # 壓縮程度越高(值越大)，保留的高頻越少\n",
    "        freq_scale = 1.0 - compression_level\n",
    "        \n",
    "        # 應用高頻增強\n",
    "        enhanced = x + attn_mask * x_dct * freq_scale\n",
    "        return self.conv_out(enhanced)\n",
    "\n",
    "# 頻率感知塊\n",
    "class FrequencyAwareBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.dct_layer = DCTLayer(block_size=8)\n",
    "        self.freq_conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.freq_attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // 4, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // 4, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, compression_level):\n",
    "        # 獲取頻率表示\n",
    "        x_dct = self.dct_layer(x)\n",
    "        \n",
    "        # 確保x_dct與x具有相同的空間尺寸\n",
    "        if x_dct.shape[2:] != x.shape[2:]:\n",
    "            x_dct = F.interpolate(x_dct, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "        x_freq = self.freq_conv(x_dct)\n",
    "        \n",
    "        # 確保x_freq與x具有相同的空間尺寸\n",
    "        if x_freq.shape[2:] != x.shape[2:]:\n",
    "            x_freq = F.interpolate(x_freq, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 生成頻率注意力圖\n",
    "        attn = self.freq_attn(x_freq)\n",
    "        \n",
    "        # 根據compression_level調整注意力權重\n",
    "        if isinstance(compression_level, torch.Tensor) and compression_level.dim() > 0:\n",
    "            compression_level = compression_level.view(-1, 1, 1, 1)\n",
    "        \n",
    "        # 壓縮程度越高，注意力權重越低\n",
    "        attn = attn * (1.0 - compression_level) + 0.5\n",
    "        \n",
    "        # 確保attn與x_freq尺寸一致\n",
    "        if attn.shape[2:] != x_freq.shape[2:]:\n",
    "            attn = F.interpolate(attn, size=x_freq.shape[2:], mode='nearest')\n",
    "        \n",
    "        # 應用頻率注意力\n",
    "        return x + x_freq * attn\n",
    "\n",
    "# 殘差注意力塊\n",
    "class ResAttnBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, time_dim, dropout=0.1, use_freq_guide=False):\n",
    "        super().__init__()\n",
    "        # 確保組數適合通道數\n",
    "        num_groups = min(8, in_c)\n",
    "        while in_c % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "            \n",
    "        self.norm1 = nn.GroupNorm(num_groups, in_c)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.time_proj = nn.Linear(time_dim, out_c)\n",
    "        \n",
    "        # 調整 out_c 的組數\n",
    "        num_groups_out = min(8, out_c)\n",
    "        while out_c % num_groups_out != 0 and num_groups_out > 1:\n",
    "            num_groups_out -= 1\n",
    "            \n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)\n",
    "        self.attn = nn.MultiheadAttention(out_c, 4, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "        \n",
    "        # 頻率增強(可選使用)\n",
    "        self.use_freq_guide = use_freq_guide\n",
    "        if use_freq_guide:\n",
    "            self.freq_guide = FrequencyAwareBlock(out_c)\n",
    "            self.hfcm = HFCM(out_c)\n",
    "        \n",
    "    def forward(self, x, t_emb, compression_level=None):\n",
    "        h = self.norm1(x)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        # 加入時間編碼\n",
    "        t = self.time_proj(t_emb)[..., None, None]\n",
    "        h = h + t\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = self.conv2(F.silu(h))\n",
    "        \n",
    "        # 應用自注意力機制\n",
    "        b, c, hh, ww = h.shape\n",
    "        h_attn = h.view(b, c, -1).permute(0, 2, 1)\n",
    "        h_attn, _ = self.attn(h_attn, h_attn, h_attn)\n",
    "        h_attn = h_attn.permute(0, 2, 1).view(b, c, hh, ww)\n",
    "        \n",
    "        # 應用頻率增強(如果啟用)\n",
    "        if self.use_freq_guide and compression_level is not None:\n",
    "            h_attn = self.freq_guide(h_attn, compression_level)\n",
    "            h_attn = self.hfcm(h_attn, compression_level)\n",
    "        \n",
    "        return self.shortcut(x) + self.dropout(h_attn)\n",
    "\n",
    "# JPEG擴散模型\n",
    "class JPEGDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        time_dim = 256\n",
    "        self.time_embed = TimeEmbedding(time_dim)\n",
    "        \n",
    "        # 下採樣路徑\n",
    "        self.down1 = ResAttnBlock(3, 64, time_dim)\n",
    "        self.down2 = ResAttnBlock(64, 128, time_dim, use_freq_guide=True)\n",
    "        self.down3 = ResAttnBlock(128, 256, time_dim, use_freq_guide=True)\n",
    "        self.down4 = ResAttnBlock(256, 512, time_dim)\n",
    "        self.down5 = ResAttnBlock(512, 512, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 瓶頸層\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResAttnBlock(512, 1024, time_dim, use_freq_guide=True),\n",
    "            ResAttnBlock(1024, 1024, time_dim),\n",
    "            ResAttnBlock(1024, 512, time_dim, use_freq_guide=True)\n",
    "        )\n",
    "        \n",
    "        # 上採樣路徑\n",
    "        self.up1 = ResAttnBlock(1024, 512, time_dim)\n",
    "        self.up2 = ResAttnBlock(512 + 512, 256, time_dim, use_freq_guide=True)\n",
    "        self.up3 = ResAttnBlock(256 + 256, 128, time_dim, use_freq_guide=True)\n",
    "        self.up4 = ResAttnBlock(128 + 128, 64, time_dim)\n",
    "        self.up5 = ResAttnBlock(64 + 64, 64, time_dim)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.out_conv = nn.Conv2d(64, 3, 1)\n",
    "        \n",
    "    def forward(self, x, t, compression_level=None):\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # 如果未提供壓縮程度，使用時間步長代替\n",
    "        if compression_level is None:\n",
    "            compression_level = t.clone().detach()\n",
    "        \n",
    "        # 下採樣\n",
    "        d1 = self.down1(x, t_emb)  # 32x32\n",
    "        d2 = self.down2(self.pool(d1), t_emb, compression_level)  # 16x16\n",
    "        d3 = self.down3(self.pool(d2), t_emb, compression_level)  # 8x8\n",
    "        d4 = self.down4(self.pool(d3), t_emb)  # 4x4\n",
    "        d5 = self.down5(self.pool(d4), t_emb)  # 2x2\n",
    "        \n",
    "        # 瓶頸層\n",
    "        b = self.bottleneck[0](self.pool(d5), t_emb, compression_level)\n",
    "        b = self.bottleneck[1](b, t_emb)\n",
    "        b = self.bottleneck[2](b, t_emb, compression_level)\n",
    "        \n",
    "        # 上採樣 - 使用連接操作將特徵合併回來\n",
    "        u1 = self.up1(torch.cat([F.interpolate(b, scale_factor=2, mode='bilinear', align_corners=False), d5], dim=1), t_emb)\n",
    "        u2 = self.up2(torch.cat([F.interpolate(u1, scale_factor=2, mode='bilinear', align_corners=False), d4], dim=1), t_emb, compression_level)\n",
    "        u3 = self.up3(torch.cat([F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False), d3], dim=1), t_emb, compression_level)\n",
    "        u4 = self.up4(torch.cat([F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False), d2], dim=1), t_emb)\n",
    "        u5 = self.up5(torch.cat([F.interpolate(u4, scale_factor=2, mode='bilinear', align_corners=False), d1], dim=1), t_emb)\n",
    "        \n",
    "        return self.out_conv(u5)\n",
    "\n",
    "# SVD結構保持函數\n",
    "def svd_structure_preservation(x, k_ratio=0.5):\n",
    "    \"\"\"使用SVD保持主要結構特徵\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    x_flat = x.view(b, c, -1)\n",
    "    \n",
    "    # 對每個通道分別進行SVD分解\n",
    "    structure_tensors = []\n",
    "    for i in range(b):\n",
    "        channels_structure = []\n",
    "        for j in range(c):\n",
    "            # SVD分解\n",
    "            U, S, Vh = torch.linalg.svd(x_flat[i, j].view(h, w), full_matrices=False)\n",
    "            \n",
    "            # 確定保留的奇異值數量\n",
    "            k = max(1, int(min(h, w) * k_ratio))\n",
    "            \n",
    "            # 重建低秩表示\n",
    "            S_k = torch.zeros_like(S)\n",
    "            S_k[:k] = S[:k]\n",
    "            structure = torch.matmul(U, torch.matmul(torch.diag(S_k), Vh))\n",
    "            channels_structure.append(structure.unsqueeze(0))\n",
    "        \n",
    "        # 合併通道結構\n",
    "        structure_tensors.append(torch.cat(channels_structure, dim=0).unsqueeze(0))\n",
    "    \n",
    "    return torch.cat(structure_tensors, dim=0)\n",
    "\n",
    "# 相位一致性函數\n",
    "def phase_consistency(x, ref, alpha=0.7):\n",
    "    \"\"\"使用傅里葉變換的相位一致性，保持頻域特性\"\"\"\n",
    "    # FFT變換\n",
    "    x_fft = torch.fft.fft2(x)\n",
    "    ref_fft = torch.fft.fft2(ref)\n",
    "    \n",
    "    # 獲取幅度和相位\n",
    "    x_mag = torch.abs(x_fft)\n",
    "    ref_phase = torch.angle(ref_fft)\n",
    "    \n",
    "    # 融合x的幅度和ref的相位\n",
    "    real = x_mag * torch.cos(ref_phase)\n",
    "    imag = x_mag * torch.sin(ref_phase)\n",
    "    adjusted_fft = torch.complex(real, imag)\n",
    "    \n",
    "    # 逆變換\n",
    "    adjusted_img = torch.fft.ifft2(adjusted_fft).real\n",
    "    \n",
    "    # 混合原始影像和相位調整影像\n",
    "    return alpha * x + (1 - alpha) * adjusted_img\n",
    "\n",
    "# 高斯混合採樣器\n",
    "class GaussianMixtureSampler:\n",
    "    def __init__(self, model, num_timesteps=100):\n",
    "        self.model = model\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "    def sample(self, x_t, steps=100, use_phase_consistency=True, use_svd_guide=True, guidance_scale=1.0):\n",
    "        \"\"\"使用高斯混合採樣進行推理，結合頻率一致性和SVD引導\"\"\"\n",
    "        self.model.eval()\n",
    "        # 保存初始壓縮影像作為頻率一致性基準\n",
    "        original_compressed = x_t.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 從給定的噪聲影像開始\n",
    "            for i in tqdm(range(steps-1, -1, -1), desc=\"Sampling\"):\n",
    "                t = torch.full((x_t.size(0),), i, device=device).float() / self.num_timesteps\n",
    "                compression_level = t.clone()  # 壓縮程度與時間步長關聯\n",
    "                \n",
    "                # 獲取噪聲預測\n",
    "                pred_noise = self.model(x_t, t, compression_level)\n",
    "                \n",
    "                # 可選使用SVD引導\n",
    "                if use_svd_guide and i > steps // 2:\n",
    "                    # 在採樣前半部分使用SVD引導(結構保存)\n",
    "                    k_ratio = i / steps  # 隨時間步長調整重要度\n",
    "                    structure_prior = svd_structure_preservation(x_t, k_ratio)\n",
    "                    # 混合SVD結構先驗與預測\n",
    "                    guide_strength = k_ratio * 0.3  # SVD權重隨時間步長調整\n",
    "                    pred_noise = (1 - guide_strength) * pred_noise + guide_strength * (original_compressed - structure_prior)\n",
    "                \n",
    "                if i > 0:\n",
    "                    # 預測的x0\n",
    "                    x0_pred = x_t + pred_noise\n",
    "                    \n",
    "                    # 計算高斯混合模型的兩個均值\n",
    "                    # 第一個均值 - 偏向原生的預測\n",
    "                    mu1 = x0_pred * 0.9 + x_t * 0.1\n",
    "                    # 第二個均值 - 偏向更激進的預測\n",
    "                    mu2 = x0_pred * 1.1 - x_t * 0.1\n",
    "                    \n",
    "                    # 根據當前時間步長動態使用不同均值\n",
    "                    # 時間步長較大時偏向保守，較小時偏向激進\n",
    "                    p_conservative = max(0.2, min(0.8, i / steps))\n",
    "                    use_first = torch.rand(1).item() < p_conservative\n",
    "                    next_mean = mu1 if use_first else mu2\n",
    "                    \n",
    "                    # 添加適量高斯噪聲\n",
    "                    noise_scale = 0.1 * i / steps * guidance_scale\n",
    "                    x_next = next_mean + noise_scale * torch.randn_like(x_t)\n",
    "                    \n",
    "                    # 頻率一致性保持(每5步使用一次)\n",
    "                    if use_phase_consistency and i % 5 == 0:\n",
    "                        alpha = 0.6 + 0.3 * (1 - i / steps)  # 隨時間步長加強初始影像權重\n",
    "                        x_next = phase_consistency(x_next, original_compressed, alpha)\n",
    "                    \n",
    "                    x_t = x_next\n",
    "                else:\n",
    "                    # 最後一步直接使用預測的去噪結果\n",
    "                    x_t = x_t + pred_noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "# 計算評估指標\n",
    "def calculate_metrics(original, restored):\n",
    "    \"\"\"計算PSNR和SSIM指標\"\"\"\n",
    "    # 轉換到[0,1]區間\n",
    "    original_01 = (original * 0.5 + 0.5).clamp(0, 1)\n",
    "    restored_01 = (restored * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    # 計算PSNR\n",
    "    mse = F.mse_loss(original_01, restored_01)\n",
    "    psnr = -10 * torch.log10(mse)\n",
    "    \n",
    "    # 計算SSIM\n",
    "    ssim_val = ssim(original_01, restored_01, data_range=1.0)\n",
    "    \n",
    "    return psnr.item(), ssim_val.item()\n",
    "\n",
    "# 執行推理主函數\n",
    "def run_inference(model_path, test_data_path, output_path, num_samples=20, qualities=[10, 30, 50, 70]):\n",
    "    \"\"\"執行推理並產生結果\"\"\"\n",
    "    # 確保輸出目錄存在\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # 載入資料\n",
    "    test_dataloader = load_test_data(test_data_path)\n",
    "    \n",
    "    # 嘗試載入LPIPS模型\n",
    "    try:\n",
    "        lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "        use_lpips = True\n",
    "    except:\n",
    "        print(\"未能載入LPIPS模型，將跳過LPIPS評估\")\n",
    "        use_lpips = False\n",
    "        lpips_fn = None\n",
    "    \n",
    "    # 載入模型\n",
    "    model = JPEGDiffusionModel().to(device)\n",
    "    num_timesteps = 100\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"載入模型權重從Epoch {checkpoint['epoch']+1}\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(\"載入模型權重\")\n",
    "    except Exception as e:\n",
    "        print(f\"載入模型失敗: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 初始化採樣器\n",
    "    model.eval()\n",
    "    sampler = GaussianMixtureSampler(model, num_timesteps=num_timesteps)\n",
    "    \n",
    "    # 追蹤結果\n",
    "    results = {q: {'psnr_compressed': [], 'psnr_restored': [], \n",
    "                   'ssim_compressed': [], 'ssim_restored': []} for q in qualities}\n",
    "    if use_lpips:\n",
    "        for q in qualities:\n",
    "            results[q]['lpips_compressed'] = []\n",
    "            results[q]['lpips_restored'] = []\n",
    "    \n",
    "    # 處理每個測試樣本\n",
    "    for idx, (x0, _) in enumerate(tqdm(test_dataloader, desc=\"Processing test images\")):\n",
    "        if idx >= num_samples:\n",
    "            break\n",
    "            \n",
    "        x0 = x0.to(device)\n",
    "        \n",
    "        # 為這個樣本創建圖像\n",
    "        plt.figure(figsize=(len(qualities)*4+4, 8))\n",
    "        plt.subplot(2, len(qualities)+1, 1)\n",
    "        plt.imshow((x0[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 測試每個質量級別\n",
    "        for i, quality in enumerate(qualities):\n",
    "            # 進行JPEG壓縮\n",
    "            compressed = jpeg_compress(x0, quality)\n",
    "            \n",
    "            # 設定初始時間步長\n",
    "            init_t = int((100 - quality) / 100 * num_timesteps)\n",
    "            \n",
    "            # 使用擴散模型恢復\n",
    "            restored = sampler.sample(compressed, steps=init_t+1, guidance_scale=0.8)\n",
    "            \n",
    "            # 計算PSNR和SSIM\n",
    "            psnr_compressed, ssim_compressed = calculate_metrics(x0, compressed)\n",
    "            psnr_restored, ssim_restored = calculate_metrics(x0, restored)\n",
    "            \n",
    "            # 記錄結果\n",
    "            results[quality]['psnr_compressed'].append(psnr_compressed)\n",
    "            results[quality]['psnr_restored'].append(psnr_restored)\n",
    "            results[quality]['ssim_compressed'].append(ssim_compressed)\n",
    "            results[quality]['ssim_restored'].append(ssim_restored)\n",
    "            \n",
    "            # 如果可用，計算LPIPS\n",
    "            if use_lpips:\n",
    "                orig_01 = (x0 * 0.5 + 0.5).clamp(0, 1) * 2 - 1  # 轉換到[-1,1]區間供LPIPS使用\n",
    "                comp_01 = (compressed * 0.5 + 0.5).clamp(0, 1) * 2 - 1\n",
    "                rest_01 = (restored * 0.5 + 0.5).clamp(0, 1) * 2 - 1\n",
    "                \n",
    "                lpips_compressed = lpips_fn(orig_01, comp_01).item()\n",
    "                lpips_restored = lpips_fn(orig_01, rest_01).item()\n",
    "                \n",
    "                results[quality]['lpips_compressed'].append(lpips_compressed)\n",
    "                results[quality]['lpips_restored'].append(lpips_restored)\n",
    "                \n",
    "                lpips_info = f\"\\nLPIPS: {lpips_compressed:.4f}\"\n",
    "                lpips_restored_info = f\"\\nLPIPS: {lpips_restored:.4f}\"\n",
    "            else:\n",
    "                lpips_info = \"\"\n",
    "                lpips_restored_info = \"\"\n",
    "            \n",
    "            # 顯示壓縮圖像\n",
    "            plt.subplot(2, len(qualities)+1, i+2)\n",
    "            plt.imshow((compressed[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "            plt.title(f\"JPEG Q{quality}\\nPSNR: {psnr_compressed:.2f}dB\\nSSIM: {ssim_compressed:.4f}{lpips_info}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # 顯示恢復圖像\n",
    "            plt.subplot(2, len(qualities)+1, len(qualities)+i+2)\n",
    "            plt.imshow((restored[0].cpu().permute(1, 2, 0) * 0.5 + 0.5).clamp(0, 1))\n",
    "            plt.title(f\"Restored\\nPSNR: {psnr_restored:.2f}dB\\nSSIM: {ssim_restored:.4f}{lpips_restored_info}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/sample_{idx+1}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 計算和打印平均結果\n",
    "    print(\"\\n==== 結果摘要 ====\")\n",
    "    print(f\"{'質量':<10} {'PSNR (壓縮)':<15} {'PSNR (還原)':<15} {'SSIM (壓縮)':<15} {'SSIM (還原)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for q in qualities:\n",
    "        avg_psnr_comp = sum(results[q]['psnr_compressed']) / len(results[q]['psnr_compressed'])\n",
    "        avg_psnr_rest = sum(results[q]['psnr_restored']) / len(results[q]['psnr_restored'])\n",
    "        avg_ssim_comp = sum(results[q]['ssim_compressed']) / len(results[q]['ssim_compressed'])\n",
    "        avg_ssim_rest = sum(results[q]['ssim_restored']) / len(results[q]['ssim_restored'])\n",
    "        \n",
    "        print(f\"{q:<10} {avg_psnr_comp:<15.2f} {avg_psnr_rest:<15.2f} {avg_ssim_comp:<15.4f} {avg_ssim_rest:<15.4f}\")\n",
    "        \n",
    "    # 如果有LPIPS結果\n",
    "    if use_lpips:\n",
    "        print(\"\\nLPIPS 結果 (越低越好):\")\n",
    "        print(f\"{'質量':<10} {'LPIPS (壓縮)':<15} {'LPIPS (還原)':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for q in qualities:\n",
    "            avg_lpips_comp = sum(results[q]['lpips_compressed']) / len(results[q]['lpips_compressed'])\n",
    "            avg_lpips_rest = sum(results[q]['lpips_restored']) / len(results[q]['lpips_restored'])\n",
    "            print(f\"{q:<10} {avg_lpips_comp:<15.4f} {avg_lpips_rest:<15.4f}\")\n",
    "    \n",
    "    # 繪製性能提升圖\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # PSNR提升\n",
    "    plt.subplot(1, 3, 1)\n",
    "    psnr_gains = [sum(results[q]['psnr_restored']) / len(results[q]['psnr_restored']) - \n",
    "                  sum(results[q]['psnr_compressed']) / len(results[q]['psnr_compressed']) \n",
    "                  for q in qualities]\n",
    "    plt.bar(qualities, psnr_gains)\n",
    "    plt.title('PSNR提升(dB)')\n",
    "    plt.xlabel('JPEG質量')\n",
    "    plt.ylabel('提升(dB)')\n",
    "    \n",
    "    # SSIM提升\n",
    "    plt.subplot(1, 3, 2)\n",
    "    ssim_gains = [sum(results[q]['ssim_restored']) / len(results[q]['ssim_restored']) - \n",
    "                  sum(results[q]['ssim_compressed']) / len(results[q]['ssim_compressed']) \n",
    "                  for q in qualities]\n",
    "    plt.bar(qualities, ssim_gains)\n",
    "    plt.title('SSIM提升')\n",
    "    plt.xlabel('JPEG質量')\n",
    "    plt.ylabel('提升')\n",
    "    \n",
    "    # LPIPS改善 (如果可用)\n",
    "    if use_lpips:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        lpips_gains = [sum(results[q]['lpips_compressed']) / len(results[q]['lpips_compressed']) - \n",
    "                       sum(results[q]['lpips_restored']) / len(results[q]['lpips_restored']) \n",
    "                       for q in qualities]\n",
    "        plt.bar(qualities, lpips_gains)\n",
    "        plt.title('LPIPS改善')\n",
    "        plt.xlabel('JPEG質量')\n",
    "        plt.ylabel('改善(值越高越好)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_path}/performance_gains.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n所有結果已保存至 {output_path} 目錄\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置參數\n",
    "    model_path = \"best_jpeg_diffusion.pth\"  # 預訓練模型路徑\n",
    "    test_data_path = \"./data\"  # 測試數據路徑\n",
    "    output_path = \"./inference_results\"  # 輸出路徑\n",
    "    num_samples = 20  # 測試樣本數量\n",
    "    qualities = [10, 30, 50, 70]  # 測試的JPEG壓縮質量\n",
    "    \n",
    "    # 執行推理\n",
    "    run_inference(model_path, test_data_path, output_path, num_samples, qualities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
